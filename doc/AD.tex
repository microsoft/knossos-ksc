
\documentclass[sigplan,review]{acmart}
\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{color}
\usepackage{proof}
\usepackage{url}
\usepackage{stmaryrd}  % double brackets
\usepackage{listings}
\lstset{
  mathescape=true
}
%\usepackage{amssymb}  % Double definition of Bbbk without this
\usepackage{enumitem}
\setlist[itemize,1]{itemsep=4pt}

\renewcommand{\arraystretch}{1.5}

\newcommand{\ksc}{\texttt{ksc}}

% The cateorical language
\newcommand{\KC}[2]{{\mathcal C}\lb \, #1 \, \rb\,#2}
\newcommand{\CatLang}{$\mathcal CL$}
\newcommand{\clcomp}{\, ; \,}   % CL composition
\newcommand{\ck}[1]{{\mathcal K}(#1)}   % CL constant
\newcommand{\cid}{\mathcal{I}}   % CL function
\newcommand{\cprune}[2]{\mathcal{P}[#1/#2]}   % CL function
\newcommand{\cfun}[1]{{\mathcal F}(#1)}   % CL function
\newcommand{\clet}[3]{{\mathcal L}(#1,#2,#3)}   % CL let
\newcommand{\cbuild}[3]{{\mathcal B}(#1,#2,#3)}   % CL build
\newcommand{\cif}[3]{\mathcal{IF}(#1,#2,#3)}   % CL conditional
\newcommand{\ctupx}{\gtrdot}   % CL tuple extension (terms)
\newcommand{\ctupxt}{\gtrdot}   % CL tuple extension (types)
\newcommand{\capply}{\diamond}   % CL application

\newcommand{\deriv}{\partial}  % The symbol we use for ``derivative''
   % Was \nabla\!, but mathematicans hate that

\renewcommand{\to}{\rightarrow}    % ->
\newcommand{\linto}{\multimap}     % -o
\newcommand{\grad}[1]{\nabla_S\lb #1 \rb}  % grad[#1]
\newcommand{\gradf}[1]{\deriv\hspace{-0.15mm} #1}  % Full Jacobian
\newcommand{\gradft}[1]{\deriv_R#1}  % Full reverse (transposed) Jacobian
\newcommand{\tangent}[1]{\delta #1}

\newcommand{\swap}{\Phi}    % Vector of pairs to pair of vectors etc

% Tuple versions
\newcommand{\targ}[1]{\overline{#1}}
\newcommand{\tgrad}[1]{\overline{\nabla}_S\lb #1 \rb}  % grad[#1]
\newcommand{\tgradf}[1]{\overline{\gradf{#1}}}  % Full Jacobian
\newcommand{\tgradft}[1]{\overline{\gradft{#1}}}  % Full Jacobian
\newcommand{\tlmtrans}[1]{#1^{\overline{\top}}}   % Transpose (takes an argument)
\newcommand{\tlmapply}{\;\overline{\odot}\;}      % Infix application\newcommand{\lmcomp}{\,\circ\,}   % Infix composition
\newcommand{\tlmvcat}{\;\overline{\lmvcat}\;}         % Infix pairing
\newcommand{\tlmhcat}{\;\overline{\lmhcat}\;}         % Infix pairing

% Polymorphic tuple versions
\newcommand{\oarg}[1]{\overline{#1}}
\newcommand{\ograd}[1]{\overline{\nabla}_a\lb #1 \rb}  % grad[#1]
\newcommand{\ogradf}[1]{\overline{\gradf{#1}}}  % Full Jacobian
\newcommand{\ogradft}[1]{\overline{\gradft{#1}}}  % Full Jacobian
\newcommand{\olmtrans}[1]{#1^{\overline{\top}}}   % Transpose (takes an argument)
\newcommand{\olmapply}{\;\overline{\odot}\;}      % Infix application\newcommand{\lmcomp}{\,\circ\,}   % Infix composition
\newcommand{\olmapplyR}{\;\overline{\odot}_R\;}      % Infix application\newcommand{\lmcomp}{\,\circ\,}   % Infix composition
\newcommand{\olmvcat}{\;\overline{\lmvcat}\;}         % Infix pairing
\newcommand{\olmhcat}{\;\overline{\lmhcat}\;}         % Infix pairing

\newcommand{\fwdDf}[1]{\mathit{fwd\$}#1}  % Forward derivative, f'
\newcommand{\revDf}[1]{\mathit{rev\$}#1}  % Reverse derivative, f`

% With splitting
\newcommand{\revfDf}[1]{\mathit{revf\$}#1}  % Reverse derivative, f`
\newcommand{\revrDf}[1]{\mathit{revr\$}#1}  % Reverse derivative, f`


\newcommand{\lb}{\llbracket}
\newcommand{\rb}{\rrbracket}
\newcommand{\sel}[2]{\pi_{#1,#2}}
\newcommand{\fst}[1]{\pi_1(#1)}
\newcommand{\snd}[1]{\pi_2(#1)}
\newcommand{\iffun}{\mathit{if}}
\newcommand{\buildfun}{\mathit{build}}
\newcommand{\sumfunname}{\mathit{sum}}   % Wanted to allow either \Sigma e or sum(e)
\newcommand{\sumfun}[1]{\sumfunname(#1)}   % Wanted to allow either \Sigma e or sum(e)
\newcommand{\sizefun}{sz}
\newcommand{\deltafun}[2]{\delta(#1,#2)}
\newcommand{\indexfunname}{\mathit{ixR}} %% Reversed from current code
\newcommand{\indexfun}[2]{\indexfunname(#2,#1)} % Vector indexing  \indexfun{v}{i}
\renewcommand{\d}[1]{\delta_{\mathit{#1}}}   % delta_x etc
\renewcommand{\vector}[1]{\mathit{Vec}\;#1}   % the type (Vec t)

\newcommand{\tcolon}{\!:\!}
\newcommand{\typ}[2]{#1 \tcolon #2}  % x:S, with less horizontal whitespace

\renewcommand{\dot}{.\,}               % dot with some space after
\newcommand{\real}{\mathbb{R}}       % R, the reals11
\newcommand{\nat}{\mathbb{N}}        % N, the natural numbers
\newcommand{\bool}{\mathbb{B}}        % B, the booleans
\newcommand{\darrow}{\Rightarrow}    % =>

% Linear maps
\newcommand{\dotprod}{\bullet}    % Dot-product of vectors
\newcommand{\lmcomp}{\,\circ\,}   % Infix composition
\newcommand{\lmvcat}{\times}         % Infix pairing
\newcommand{\lmvcatv}[1]{{\mathcal V}(#1)}         % Vector version
\newcommand{\lmhcat}{\bowtie}        % Infix join
\newcommand{\lmhcatv}[1]{{\mathcal H}(#1)}         % Vector version
\newcommand{\lmapply}{\,\odot\,}      % Infix application\newcommand{\lmcomp}{\,\circ\,}   % Infix composition
\newcommand{\lmapplyR}{\,\odot_R\,}      % Infix application\newcommand{\lmcomp}{\,\circ\,}   % Infix composition
\newcommand{\lmadd}{\oplus}        % Infix join
\newcommand{\lmtrans}[1]{#1^{\top}}   % Transpose (takes an argument)
\newcommand{\lmzero}{\mathbf{0}}     % 0
\newcommand{\lmone}{\mathbf{1}}      % 1
\newcommand{\lmscalar}[1]{{\mathcal S}(#1)}      % S(k)
\newcommand{\lmlam}{{\mathcal L}}      % L(k)
\newcommand{\lmlamt}{{\mathcal L'}}     % Transposed lambda

% \newcommand{\simon}[1]{{\color{red}SPJ: #1}}
% \newcommand{\tom}[1]{{\color{red}TE: #1}}
% \newcommand{\awf}[1]{{\color{teal}AWF: #1}}
\newcommand{\simon}[1]{}
\newcommand{\tom}[1]{}
\newcommand{\awf}[1]{}

\begin{document}
\title{Working notes on Automatic differentation}
\subtitle{\today{} --- not for circulation}
\author{Tom Ellis}
\author{Simon Peyton Jones}
\author{Andrew Fitzgibbon}

\maketitle

% ------------------------------------
\section{The language}

\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
$$
      \begin{array}{rcll}
        \multicolumn{4}{l}{\mbox{\bf Atoms}} \\
        f,g,h & ::= & \multicolumn{2}{l}{\mbox{Function}} \\
        x,y,z & ::= & \multicolumn{2}{l}{\mbox{Local variable (lambda-bound or let-bound)}} \\
        k & ::= & \multicolumn{2}{l}{\mbox{Literal constants}} \\
        \\
        \multicolumn{4}{l}{\mbox{\bf Terms}} \\
        \mathit{pgm} & ::= & \mathit{def}_1 \ldots \mathit{def}_n \\
        \mathit{def} & ::= & f(x) = e \\
        e & ::= & k & \mbox{Constant} \\
          & |   & x & \mbox{Local variable} \\
          & |   & f(e) & \mbox{Function call} \\
          & |   & (e_1,e_2) & \mbox{Pair} \\
          & |   & \lambda x \dot e & \mbox{Lambda} \\
          & |   & e_1 \; e_2 & \mbox{Application} \\
          & |   & \mbox{\lstinline|let $\;x$ = $e_1\;$ in $\;e_2$|} \\
          & |   & \mbox{\lstinline|if $\;b\;$ then $\;e_1\;$ else $\;e_2$|} \\
        \\
        \multicolumn{4}{l}{\mbox{\bf Types}} \\
        \tau & ::= & \nat & \mbox{Natural numbers} \\
        & | & \real & \mbox{Real numbers} \\
        & | & (\tau_1, \tau_2) & \mbox{Pairs} \\
        & | & \vector{\tau} & \mbox{Vectors} \\
        & | & \tau_1 \to \tau_2 & \mbox{Functions} \\
        & | & \tau_1 \linto \tau_2 & \mbox{Linear maps} \\
      \end{array}
 $$
\end{minipage}}
\caption{Syntax of the language} \label{fig:syntax}
\end{figure}
This paper is about automatic differentiation of functions, so we must be precise about
the language in which those functions are written.

The syntax of our language is given in Figure~\ref{fig:syntax}.
Note that
\begin{itemize}
\item  Variables are divided into \emph{functions}, $f,g,h$; and \emph{local variables}, $x,y,z$,
  which are either function arguments or let-bound.
\item
  The language has a first order sub-language.  Functions are defined at top level;
  functions always appear in a call, never (say) as an argument to a
  function; in a call $f(e)$, the function $f$ is always a
  top-level-defined function, never a local variable.  \awf{at some point we should say where this restriction is needed}

\item Functions have exactly one argument. If you want more than one, pass a pair.

\item Pairs are built-in, with selectors $\sel{1}{2}, \sel{2}{2}$.
  In the real implementation, pairs are generalised to $n$-tuples, and we often do so informally here.

\item Conditionals are a language construct. \simon{Treating ``if'' as a function
  just didn't work; in particular $\gradf{if}$ needed a linear-map version of
  ``if'' and once we have that we might as well build ``if'' in.  Anyway,
  conditionals are very fundamental, so it's unsurprising.}

\item Let-bindings are non-recursive. For now, at least, top-level
  functions are also non-recursive.  \simon{I think that top-level
    recursive functions might be OK, but I don't want to think about
    that yet.}

\item Lambda expressions and applications are present, so the language
  is higher order.  AD will only accept a subset of the language, in
  which lambdas appear only as an argument to $\buildfun$.  But the
  \emph{output} of AD may include lambdas and application, as we shall see.
  \end{itemize}

\subsection{Built in functions}

\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
{\bf Built-in functions}
$$
\begin{array}{rcll}
  (+) & :: & (\real,\real) \to \real \\
  (*) & :: & (\real,\real) \to \real \\
  \sel{1}{2} & :: & (t_1,t_2) \to t_1 & \mbox{Selection} \\
  \sel{2}{2} & :: & (t_1,t_2) \to t_2 & \mbox{..ditto..} \\
%  \deltafun & :: & Field \; t \darrow (\nat,\; \nat) \to t & \mbox{Delta-function} \\
  \buildfun & :: & (n::\nat,\; \nat \to t) \to \vector{t} & \mbox{Vector build} \\
  \indexfunname & :: & (\nat, \; \vector{t}) \to t & \mbox {Indexing (NB arg order)} \\
  \sumfunname & :: & \vector{t} \to t & \mbox{Sum a vector} \\
  \sizefun & :: & \vector{t} \to \nat & \mbox{Size of a vector} \\
\end{array}
$$
\\[3mm]
{\bf Derivatives of built-in functions}
      $$
      \begin{array}{rcl}
        \gradf{+}      & :: & (\real,\real) \to ((\real,\real) \linto \real) \\
        \gradf{+}(x,y) & = & \lmone \lmhcat \lmone \\[2mm]
        \gradf{*}      & :: & (\real,\real) \to ((\real,\real) \linto \real) \\
        \gradf{*}(x,y) & = & \lmscalar{y} \lmhcat \lmscalar{x} \\[2mm]
        \gradf{\sel{1}{2}}      & :: & (t,t) \to ((t,t) \linto t) \\
        \gradf{\sel{1}{2}}(x) & = & \lmone \lmhcat \lmzero \\[2mm]
        \gradf{\indexfunname} & :: & (\nat, \;\vector{t}) \to ((\nat, \; \vector{t}) \linto t) \\
        \gradf{\indexfunname}(i,v) & = & \lmzero \lmhcat
        \lmhcatv{\buildfun( \sizefun(v),
              \lambda j. \mbox{\lstinline|if|}\, i=j \,\mbox{\lstinline|then|}\, \lmone  \, \mbox{\lstinline|else|}\, \lmzero )} \\[2mm]
         \gradf{\sumfunname} & :: & \vector{\real} \to (\vector{\real} \linto \real) \\
         \gradf{\sumfun{v}} & = & lmhcatv{\buildfun( \sizefun(v),\; \lambda i. \lmone )} \\[2mm]
        \ldots
        \end{array}
$$
\end{minipage}}
\caption{Built-in functions} \label{fig:built-in}
\end{figure}

The language has built-in functions shown in Figure~\ref{fig:built-in}.

We allow ourselves to write functions infix where it is convenient.
Thus $e_1 + e_2$ means the call $+(e1,e2)$, which applies the function $+$ to
the pair $(e_1,e_2)$.  (So, like all other functions, $(+)$ has one argument.)
Similarly the linear map $m_1 \lmvcat m_2$ is short for $\lmvcat(e_1,e_2)$.

We allow ourselves to write vector indexing $\indexfun{a}{i}$ using square brackets, thus $a[i]$.

Multiplication and addition are overloaded to work on any suitable type.
On vectors they work element-wise; if you want dot-product you have to program it.


% The Kronecker delta $\deltafun$ is defined thus:
% $$
% \begin{array}{rcll}
%   \deltafun(i,j) & = & 1 & \mbox{if $i=j$} \\
%  & = & 0 & \mbox{otherwise}
% \end{array}
% $$

\subsection{Vectors}

The language supports one-dimensional vectors, of type $\vector{T}$,
whose elements have type $T$ (Figure~\ref{fig:syntax}).
A matrix can be represented as a vector of vectors.

Vectors are supported by the following built-in functions (Figure~\ref{fig:built-in}):
\begin{itemize}
\item $\buildfun :: (\nat,\; \nat \to t) \to \vector{t}$ for vector construction.
\item $\indexfunname :: (\nat,\; \vector{t}) \to t$ for indexing.  Informally we allow ourselves to write
  $v[i]$ instead of $\indexfun{v}{i}$.
\item $\sumfunname :: \vector{\real} \to \real$ to add up the elements of a vector.
We specifically do not have a general, higher order, fold operator;
we say why in Section~\ref{sec:gen-fold}.
\item $\sizefun :: \vector{t} \to \nat$ takes the size of a vector.
\item Arithmetic functions $(*), (+)$ etc are overloaded to work over vectors, always elementwise.
\end{itemize}


\simon{Do we need scan?  Or (specialising to $(+)$) cumulative sum?}

% ------------------------------------
\section{Linear maps and differentiation}

If $f : S \to T$, then its derivative $\gradf{f}$ has type
$$\gradf{f} : S \to (S \linto T)$$
where $S \linto T$ is the type of \emph{linear maps} from $S$ to $T$.
That is, at some point $p:S$, $\gradf{f}(p)$ is a linear map
that is a good approximation of $f$ at $p$.

By ``a good approximation of $f$ at $p$'' we mean this:
$$
\forall p:S.\; f( p + \d{p} ) \approx f(p) + \gradf{f}(p) \lmapply \d{p}
$$
Here the operation $(\lmapply)$ is linear-map application: it takes a
linear map $S \linto T$ and applies it to an argument of type $S$,
giving a result of type $T$ (Figure~\ref{fig:linear-maps}).

The linear maps from $S$ to $T$ are a subset of the
functions from $S$ to $T$.  We characterise linear maps more precisely
in Section~\ref{sec:lin-maps}, but
a good intuition can be had for functions $g : \real^2 \to \real$.
This function defines a curvy surface $z = g(x,y)$.  Then a linear map
of type $\real^s \linto \real$ is a plane, and $\gradf{g}(p_x,p_y)$ is
the plane that best approximates $g$ near $(p_x,p_y)$, that is a tangent plane
passing through $z = g(p_x, p_y)$

\subsection{Linear maps} \label{sec:lin-maps}

\begin{figure*}
\fbox{\begin{minipage}{\textwidth}
  $$
  \begin{array}{rr@{\hspace{2mm}}c@{\hspace{2mm}}ll}
      & \multicolumn{3}{l}{\mbox{Operator \hspace{2em} Type}} & \mbox{Matrix interpretation} \\
      & &&& \mbox{where $s = \real^m$, and $t = \real^n$} \\
\hline
    \mbox{Apply} & (\lmapply) & : & (s \linto t) \to \tangent{s} \to \tangent{t}
           & \mbox{Matrix/vector multiplication} \\
    \mbox{Reverse apply} & (\lmapplyR) & : & \tangent{t} \to (s \linto t) \to \tangent{s}
           & \mbox{Vector/matrix multiplication} \\[2mm]
    \mbox{Compose} & (\lmcomp) & : & (s \linto t,\; r \linto s) \to (r \linto t)
           & \mbox{Matrix/matrix multiplication} \\
    \mbox{Sum}   & (\lmadd) & : & (s \linto t,\; s \linto t) \to (s \linto t)
           & \mbox{Matrix addition } \\
    \mbox{Zero}  & \lmzero & : & s \linto t & \mbox{Zero matrix}\\
    \mbox{Unit}  & \lmone  & : & s \linto s & \mbox{Identity matrix (square)}\\
    \mbox{Scale} & \lmscalar{\cdot} & : & \real \to (s \linto s) \\
    \mbox{VCat}      & (\lmvcat) & : & (s \linto t_1,\; s \linto t_2) \to (s \linto (t_1,t_2))
           & \mbox{Vertical juxtaposition} \\
    \mbox{VCatV}      & \lmvcatv{\cdot} & : & \vector{(s \linto t)} \to (s \linto \vector{t})
           & \mbox{...vector version} \\
    \mbox{HCat}  & (\lmhcat) & : & (t_1 \linto s,\; t_2 \linto s) \to ((t_1,t_2) \linto s)
           & \mbox{Horizontal juxtaposition} \\
    \mbox{HCatV}  & \lmhcatv{\cdot} & : & \vector{(t \linto s)} \to (\vector{t} \linto s)
           & \mbox{...vector version} \\

    \mbox{Transpose} & \lmtrans{\cdot} & : & (s \linto t) \to (t \linto s) & \mbox{Matrix transpose} \\[2mm]
%    \\
    \multicolumn{5}{l}{\mbox{\bf NB: We expect to have only $\lmlam / \lmlamt$ but not both}} \\
    \mbox{Lambda} & \lmlam & : & (\nat \to (s \linto t)) \to (s \linto (\nat \to t)) \\
    \mbox{TLambda} & \lmlamt & : & (\nat \to (t \linto s)) \to ((\nat \to t) \linto s) & \mbox{Transpose of $\lmlam$} \\
  \end{array}
  $$
\end{minipage}}
  \caption{Operations over linear maps} \label{fig:linear-maps}
\end{figure*}

\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
{\bf Semantics of linear maps}
  $$
  \begin{array}{r@{\;}c@{\;}l}
    (m_1 \lmcomp m_2) \lmapply x  & = & m_1 \lmapply (m_2 \lmapply x) \\
    (m_1 \lmvcat m_2) \lmapply x  & = & (m_1 \lmapply x, m_2 \lmapply x) \\
    (m_1 \lmhcat m_2)  \lmapply (x_1,x_2) & = & (m_1 \lmapply x_1) + (m_2 \lmapply x_2) \\
    (m_1 \lmadd m_2)  \lmapply x & = & (m_1 \lmapply x) + (m_2 \lmapply x) \\
    \lmzero \lmapply x  & = & 0 \\
    \lmone \lmapply x & = & x \\
    \lmscalar{k} \lmapply x & = & k * x \\
    \lmvcatv{m} \lmapply x  & = & build( \sizefun(m), \lambda i. m[i] \lmapply x) \\
    \lmhcatv{m} \lmapply x  & = & \Sigma_i\, (m[i] \lmapply x[i]) \\
   \lmlam(f) \lmapply x & = & \lambda i\dot (f\; i) \lmapply x \\
   \lmlamt(f) \lmapply g & = & \Sigma_i (f \;i) \lmapply g(i) \\
   \end{array}
  $$
\\[3mm]
  {\bf Properties of linear maps}
  $$
  \begin{array}{rcl}
    \lmzero \lmcomp m & = & \lmzero \\
    m \lmcomp \lmzero & = & \lmzero \\
    \lmone \lmcomp m & = & m \\
    m \lmcomp \lmone & = & m \\
    m \lmadd \lmzero & = & m \\
    \lmzero \lmadd m & = & m \\
    m \lmcomp (n_1 \lmhcat n_2) & = & (m \lmcomp n_1) \lmhcat (m \lmcomp n_2) \\
    (m_1 \lmvcat m_2) \lmcomp n & = & (m_1 \lmcomp n) \lmvcat (m_2 \lmcomp n) \\
    (m_1 \lmhcat m_2) \lmcomp (n_1 \lmvcat n_2) & = & (m_1 \lmcomp n_1) \lmadd (m_2 \lmcomp n_2) \\
    \lmscalar{k_1} \lmcomp \lmscalar{k_2} & = & \lmscalar{ k_1 * k_2 } \\
    \lmscalar{k_1} \lmadd \lmscalar{k_2} & = & \lmscalar{ k_1 + k_2 } \\
  \end{array}
  $$
    \end{minipage}
    }
    \caption{Linear maps: semantics and properties} \label{fig:lm-laws}
\end{figure}

A \emph{linear map}, $m \tcolon S \linto T$, is a function from $S$ to $T$,
satisfying these two properties:
$$
\begin{array}{lrrcl}
(LM1) & \forall \typ{x,y}{S} &  m \lmapply (x+y) & = & m \lmapply x + m \lmapply y \\
(LM2) & \forall \typ{k}{\real}, \typ{x}{S} & k * (m \lmapply x) & = & m \lmapply (k * x)
\end{array}
$$
Here $(\lmapply) : (s \linto t) \to (s \to t)$ is an operator that applies a linear map $(s \linto t)$
to an argument of type $s$.
The type $s \linto t$ is a type in the language (Figure~\ref{fig:syntax}).

Linear maps can be \emph{built and consumed} using the
operators in (see Figure~\ref{fig:linear-maps}).
Indeed, you should think of linear maps as an \emph{abstract type}; that is,
you can \emph{only} build or consume linear maps with
the operators in Figure~\ref{fig:linear-maps}.  We might \emph{represent} a linear map in a variety
of ways, one of which is as a matrix (Section~\ref{sec:matrix-rep}).

\subsubsection{Semantics of linear maps}

The \emph{semantics} of a linear map is completely specified by saying
what ordinary function it corresponds to; or, equivalently, by how it
behaves when applied to an argument by $(\lmapply)$.  The semantics of
each form of linear map are given in Figure~\ref{fig:lm-laws}

\subsubsection{Properties of linear maps}

Linear maps satisfy \emph{properties} given in Figure~\ref{fig:lm-laws}.
Note that $(\lmcomp)$ and $\lmadd$ behave like multiplication and
addition respectively.

These properties can readily be proved from the semantics.  To prove two linear maps are equal,
we must simply prove that they give the same result when applied to any argument.
So, to prove that $\lmzero \lmcomp m = m$, we choose an arbitrary $x$ and reason thus:
$$\begin{array}{cll}
  \multicolumn{3}{l}{(\lmzero \lmcomp m) \lmapply x} \\
  = & \lmzero \lmapply (m \lmapply x) &  \mbox{\{semantics of $(\lmcomp$)\}} \\
  = & 0                  & \mbox{\{semantics of $\lmzero$\}} \\
  = & \lmzero \lmapply x & \mbox{\{semantics of $\lmzero$ backwards\}}
\end{array}
$$

Note that the property
$$
(m_1 \lmhcat m_2) \lmcomp (n_1 \lmvcat n_2) \; = \;  (m_1 \lmcomp n_1) \lmadd (m_2 \lmcomp n_2) \\
$$
is the only reason we need the linear map $(\lmadd)$.

{\bf Theorem}: $\forall (m \tcolon S \linto T).\, m \lmapply 0 = 0$.  That is, all linear maps
pass through the origin.  {\bf Proof}: property (LM2) with $k=0$.  Note that the function
$\lambda x. x+4$ is not a linear map; its graph is a staight line, but it does not
go through the origin.

\subsection{Vector spaces}

Given a linear map $m \tcolon S \linto T$, we expect both $S$ and $T$ to be
a \emph{vector space with dot product} (aka inner product space\footnote{\url{https://en.wikipedia.org/wiki/Vector_space}}).
A vector space with dot product $V$ has:
\begin{itemize}
\item \emph{Vector addition} $(+_V) : V \to V \to V$.
\item \emph{Zero vector} $0_V : V$.
\item \emph{Scalar multiplication} $(*_V) : \real \to V \to V$
\item \emph{Dot-product} $(\dotprod_V) : V \to V \to \real$.
\end{itemize}
We omit the $V$ subscripts when it is clear which $(*)$, $(+)$, $(\dotprod)$ or $0$ is intended.

These operations must obey the laws of vector spaces
$$
\begin{array}{rcl}
v_1 + (v_2 + v_3) & = & (v_1 + v_2) + v_3 \\
v_1 + v_2 & = & v_2 + v_1 \\
v + 0 & = & 0 \\
0 * v & = & 0 \\
1 * v & = & v \\
r_1 * ( r_2 * v) & = & (r_1 * r_2) * v \\
r * (v_1 + v_2) & = & (r * v_1) + (r * v_2) \\
(r_1 + r_2) * v & = & (r_1 * v) + (r_2 * v) \\
\end{array}
$$

\subsubsection{Building vector spaces}

What types are vector spaces?  Look the syntax of types in Figure~\ref{fig:syntax}.
\begin{itemize}
  \item The real numbers $\real$ is a vector space, using the standard $+$ and $*$ for reals; and $\dotprod_{\real} = *$.
  \item If $V$ is a vector space then $\vector{V}$ is a vector space, with
    \begin{itemize}
      \item $v_1 + v_2$ is vector addittion
      \item $r * v$ multiplies each element of the vector $v$ by the real $r$.
      \item $v_1 \dotprod v_2$ is a the usual vector dot-product.
    \end{itemize}
    We often write $\vector{\real}$ as $\real^N$.
  \item If $V_1$ and $V_2$ are vector spaces, then the product space $(V_1, V_2)$ is a vector space
    \begin{itemize}
      \item $(v_1,v_2) +(w_1,w_2) = (v_1 + w_1, v_2 + w_2)$.
      \item $r * (v_1,v_2) = (r*v_1, r*v_2)$
      \item $(v_1,v_2) \dotprod (w_1,w_2) = (v_1 \dotprod w_1) + (v_2 \dotprod w_2)$.
    \end{itemize}
\end{itemize}
In all cases the necessary properties of the operations (associativity, distribution etc) are easy to prove.

\subsection{Transposition}

\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
  {\bf Laws for transposition of linear maps}
  $$
  \begin{array}{rcll}
    \lmtrans{(m_1 \lmcomp m_2)} & = & \lmtrans{m_2} \lmcomp \lmtrans{m_1} & \mbox{Note reversed order!}\\
    \lmtrans{(m_1 \lmvcat m_2)} & = & \lmtrans{m_1} \lmhcat \lmtrans{m_2} \\
    \lmtrans{(m_1 \lmhcat m_2)} & = & \lmtrans{m_1} \lmvcat \lmtrans{m_2} \\
    \lmtrans{(m_1 \lmadd m_2)} & = & \lmtrans{m_1} \lmadd \lmtrans{m_2} \\
    \lmtrans{\lmzero} & = & \lmzero \\
    \lmtrans{\lmone} & = & \lmone \\
    \lmtrans{\lmscalar{k}} & = & \lmscalar{k} \\
    \lmtrans{(\lmtrans{m})} & = & m \\
    \lmtrans{\lmvcatv{v}} & = & \lmhcatv{map\;\lmtrans{(\cdot)}\,v} \\
    \lmtrans{\lmhcatv{v}} & = & \lmvcatv{map\;\lmtrans{(\cdot)}\,v} \\
   \lmtrans{\lmlam(\lambda i. m)} & = & \lmlamt(\lambda i. \lmtrans{m}) \\
   \lmtrans{\lmlamt(\lambda i. m)} & = & \lmlam(\lambda i. \lmtrans{m}) \\
  \end{array}
  $$
  {\bf Laws for reverse-application}
  $$
  \begin{array}{r@{\;}c@{\;}l}
    r \lmapplyR m & = & \lmtrans{m} \lmapply r \qquad \mbox{By definition} \\[1mm]
    r \lmapplyR (m_1 \lmcomp m_2) & = & (r \lmapplyR m_1) \lmapplyR m_2 \\
    (r_1,r_2) \lmapplyR (m_1 \lmvcat m_2) & = & (r_1 \lmapplyR m_1) + (r_2 \lmapplyR m_2) \\
    r \lmapplyR (m_1 \lmhcat m_2) & = & (r \lmapplyR m_1,\; r \lmapplyR m_2) \\
    r \lmapplyR (m_1 \lmadd m_2) & = & (r \lmapplyR m_1) + (r \lmapplyR m_2) \\
    r \lmapplyR \lmzero & = & 0 \\
    r \lmapplyR \lmone & = & r \\
    r \lmapplyR \lmscalar{k} & = & k*r \\
    r \lmapplyR \lmtrans{m} & = & m \lmapply r \\
    r \lmapplyR \lmvcatv{v} & = & \Sigma_i\, (r[i] \lmapplyR v[i]) \\
    r \lmapplyR \lmhcatv{v} & = & \buildfun( \sizefun(v), \lambda i. r \lmapplyR m[i]) \\
%   \lmtrans{\lmlam(\lambda i. m)} & = & \lmlamt(\lambda i. \lmtrans{m}) \\
%   \lmtrans{\lmlamt(\lambda i. m)} & = & \lmlam(\lambda i. \lmtrans{m}) \\
  \end{array}
  $$
    \end{minipage}
    }
    \caption{Laws for transposition} \label{fig:lm-trans}
\end{figure}

For any linear map $m \tcolon S \linto T$ we can produce its transpose $\lmtrans{m} : T \linto S$.
Despite its suggestive type, the transpose is \emph{not} the inverse of $m$!  (In the world of matrices,
the transpose of a matrix is not the same as its inverse.)

\begin{definition}
Given a linear map $m : S \linto T$, its \emph{transpose} $\lmtrans{m} : T \linto S$
is defined by the following property:
$$
(TP)\quad \forall s\tcolon S,\, t \tcolon T.\;\;  (\lmtrans{m} \lmapply t) \dotprod s =  t \dotprod (m \lmapply s)
$$
\end{definition}
\noindent
This property \emph{uniquely} defines the transpose, as the following theorem shows:
\begin{theorem} \label{thm:trans-eq}
  If $m_1$ and $m_2$ are linear maps satisfying
  $$\forall s\, t.\; (m_1 \lmapply s) \dotprod t
                              = (m_2 \lmapply s) \dotprod t$$
  then $m_1$ = $m_2$
\end{theorem}
\begin{proof}
  It is a property of dot-product that if $v_1 \dotprod x = v_2 \dotprod x$ for every
  $x$, then $v_1 = v_2$.  (Just use a succession of one-hot vectors for $x$,
  to pick out successive components of $v_1$ and $v_2$.) So
  (for every $t$):
$$
\begin{array}{ll}
\forall s\, t.\; (m_1 \lmapply s) \dotprod t
                              = (m_2 \lmapply s) \dotprod t \\
\Rightarrow \;\; \forall s. \; m_1 \lmapply s = m_2 \lmapply s
\end{array}
$$
and that is the definition of extensional equality. So $m_1$ and $m_2$ are the same linear maps.
\end{proof}
Figure~\ref{fig:lm-trans} has a collection of laws about transposition.
These identies are readily proved using the above definition. For example, to prove that
$\lmtrans{(m_1 \lmcomp m_2)}  =  \lmtrans{m_2} \lmcomp \lmtrans{m_1}$ we may reason as follows:
$$
\begin{array}{ll}
  ((\lmtrans{m_2} \lmcomp \lmtrans{m_1}) \lmapply t) \dotprod s \\
  = (\lmtrans{m_2} \lmapply (\lmtrans{m_1} \lmapply t)) \dotprod s  & \mbox{Semantics of $(\lmcomp)$} \\
  = (\lmtrans{m_1} \lmapply t) \dotprod (m_2 \lmapply s)  & \mbox{Use (TP)} \\
  = t \dotprod (m_1 \lmapply (m_2 \lmapply s))  & \mbox{Use (TP) again} \\
  = t \dotprod ((m_1 \lmcomp m_1) \lmapply s)  & \mbox{Semantics of $(\lmcomp)$} \\
\end{array}
$$
And now the property follows by Theorem~\ref{thm:trans-eq}.

\subsection{Reverse linear-map application}

Rather than transpose the linear map (which is a rather boring operation), just replacing
one operator with another, it's easier to define a reverse-application operator for
linear maps:
$$
(\lmapplyR)  :  \tangent{t} \to (s \linto t) \to \tangent{s}
$$
It is defined by the following property:
$$
(RP) \quad \forall s\tcolon \tangent{S},\, t \tcolon \tangent{T}.\;\;
     (t \lmapplyR m) \dotprod s =  t \dotprod (m \lmapply s)
$$

\subsection{Matrix interpretation of linear maps} \label{sec:matrix-rep}

A linear map $m \tcolon \real^M \linto \real^N$ is isomorphic to a matrix $\real^{N \times M}$ with $N$ rows and $M$ columns.

Many of the operators over linear maps then have simple matrix interpetations; for example, composition of
linear maps $(\lmcomp)$ is matrix multiplication, pairing $(\lmvcat)$ is vetical juxtaposition, and so on.
These matrix interpretations are all given in the final column of Figure~\ref{fig:linear-maps}.

You might like to check that matrix transposition satisfies property (TP).

When it comes to implementation, we do not want to \emph{represent} a linear map by a matrix,
becuase a linear map $\real^M \linto \real^N$ is an $N \times M$ matrix, which is
enormous if $N = M = 10^6$, say.  The function might be very simple (perhaps even the identity
function) and taking $10^{12}$ numbers to represent it is plain silly.  So our goal will be
to \emph{avoid realising linear maps as matrices}.

\subsection{Optimisation}

In optimisation we are usually given a function $f : \real^N \to \real$, where $N$ can be large,
and asked to
find values of the input that maximises the output.  One way to do this is by \emph{gradient descent}:
start with a point $p$, make a small change to $p+\d{p}$, and so on.  From $p$ we want
to move in the direction of maximum slope.  (How \emph{far} to move in that
direction is another matter --- indeed no one knows ---
but we will concentrate on the \emph{direction} in which to move.)

Suppose $\deltafun i N$ is the one-hot N-vector with 1 in the $i$'th position and zeros elsewhere.
Then $\d{p}[i] = \gradf{f}(p) \lmapply \deltafun i N$ describes how fast the output of $f$ changes
for a change in the $i$'th input.  The direction of maximum slope is just the vector
$$\d{p} = (\d{p}[1]\; \d{p}[2] \; \ldots \; \d{p}[N])$$
How can we compute this vector?  We can simply evaluate $\gradf{f}(p) \lmapply \deltafun i N$ for each $i$.
But that amounts to running $f$ $N$ times, which is bad if N is large (say $10^6$).

Suppose that we somehow had access to $\gradft{f}$.  Then we can use property $(TP)$, setting $\d{f} = 1$
to get
$$
\forall \d{p}.\; \gradf{f}(p) \lmapply \d{p} = (\gradft{f}(p) \lmapply 1) \dotprod \d{p}
$$
Then
$$
\begin{array}{rcl}
  \d{p}[i] & = & \gradf{f}(p) \lmapply \deltafun i N \\
  & = & (\gradft{f}(p) \lmapply 1) \dotprod \deltafun i N \\
  & = & (\gradft{f}(p) \lmapply 1)[i]
\end{array}
$$
That is $\d{p}[i]$ is the $i$'th component of $\gradft{f}(p) \lmapply 1$,
so $\d{p} = \gradft{f}(p) \lmapply 1$.

That is, $\gradft{f}(p) \lmapply 1$ is the N-vector of maximum slope, the direction in which to
move if we want to do gradient descent starting at $p$.  And \emph{that} is why the transpose
is important.

\subsection{Lambdas and linear maps}

Notice the similarity between the type of $(\lmvcat)$ and the type of $\lmlam$; the latter is really
just an infinite version of the latter.
Their semantics in Figure~\ref{fig:lm-laws} are equally closely  related.

The tranpsositions of these two linear maps, $(\lmhcat)$ and $\lmlamt$, are
similarly related. \emph{But}, there is a problem with the semantics of
$\lmlamt$:
$$
   \lmlamt(f) \lmapply g \quad = \quad \Sigma_i (f \;i) \lmapply g(i) \\
$$
This is an \emph{infinite sum}, so there is something fishy about this as a semantics.


\subsection{Questions about linear maps}

\begin{itemize}
\item Do we need $\lmone$? After all $\lmscalar{1}$ does the same job.
  But asking if $k=1$ is dodgy when $k$ is a float. \awf{No, perfectly fine to ask if a
    float is 1 --- the nearby floats are far away, and there's no other float $f$ such that $\lmscalar f = \lmone$.}
  \simon{For the purposes of this paper I prefer having $\lmone$ as well; unity plays such a key role!}
\item Do these laws fully define linear maps?
\end{itemize}
Notes
\begin{itemize}
\item In practice we allow n-ary versions of $m \lmhcat n$ and $m \lmvcat n$.
\end{itemize}

% --------------------------------------
\section{AD as a source-to-source transformation}


\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
$$
\begin{array}{ll}
  \mbox{\bf Original function}   & f : S \to T \\
  & f(x) = e \\[2mm]
  \mbox{\bf Full Jacobian}       & \gradf{f}  :  S \to (S \linto T) \\
  & \mbox{\lstinline|$\gradf{f}(x)$ = let $\;\gradf{x}$ = $\lmone\;$ in $\;\grad{e}$|} \\[2mm]
  \mbox{\bf Forward derivative}  & \fwdDf{f} : (S,S) \to T \\
  & \fwdDf{f}(x,dx) = \gradf{f}(x) \lmapply  dx \\[2mm]
  \mbox{\bf Reverse derivative}  & \revDf{f} : (S,T) \to S \\
  & \revDf{f}(x,dr) = dr \lmapplyR \gradf{f}(x)
\end{array}
$$
      {\bf Differentiation of an expression} \\
      \begin{center}
        If $e : T$ then $\grad{e} : S \linto T$
      \end{center}
$$
      \begin{array}{rcll}
        \grad{k} & = & \lmzero \\
        \grad{x} & = & \gradf{x} \\
        \grad{f(e)} & = & \gradf{f}(e) \lmcomp \grad{e} \\
        \grad{(e_1,e_2)} & = & \grad{e_1} \lmvcat \grad{e_2} \\
        \grad{\mbox{\lstinline|let $\;x$ = $e_1\;$ in $\;e_2$|}}
        & = & \begin{array}[t]{l}
           \mbox{\lstinline|let $\;x\;$ = $\;e_1\;$ in|} \\
           \mbox{\lstinline|let $\;\gradf{x}\;$ = $\;\grad{e_1}\;$ in|} \\
           \mbox{\lstinline|$\grad{e_2}$|}
           \end{array} \\
%        \grad{\buildfun(e_n, \lambda i.e)} & = & \lmbuild(e_n, \lambda i. \grad{e})  \\
        \grad{\buildfun(e_n, \lambda i.e)} & = & \lmvcatv{\buildfun(e_n, \lambda i. \grad{e})}  \\
        \grad{\lambda i \dot e} & = & \lmlam(\lambda i\dot \grad{e})
      \end{array}
      $$
\end{minipage}}
\caption{Automatic differentiation} \label{fig:ad}
\end{figure}

To perform source-to-source AD of a function $f$, we follow the plan
outlined in Figure~\ref{fig:ad}.  Specifically, starting with a
function definition \lstinline|f(x) = e|:

\begin{itemize}
\item Construct the full Jacobian $\gradf{f}$, and transposed full Jacobian $\gradft{f}$,
  using the tranformations in Figure~\ref{fig:ad}\footnote{
    We consider $\gradf{f}$ and $\gradft{f}$ to be the names of two new functions.
    These names are derived from, but distinctd from $f$, rather like $f'$ or $f_1$ in mathematics.
}.
\item Optimise these two definitions, using the laws of linear maps
  in Figure~\ref{fig:lm-laws}.
\item Construct the forward derivative $\fwdDf{f}$ and reverse derivative $\revDf{f}$,
  as shown in Figure~\ref{fig:ad}\footnote{Again $\fwdDf{f}$ and $\revDf{f}$ are new names, derived from $f$}.
\item Optimise these two definitions, to eliminate all linear maps. Specifically:
  \begin{itemize}
    \item Rather than \emph{calling} $\gradf{f}$ (in, say, $\fwdDf{f}$), instead \emph{inline} it.
    \item Similarly, for each local let-binding for a linear map, of form \lstinline|let $\;\gradf{x}$ = $e\;$ in $b$|,
      inline $\gradf{x}$ at each of its occurrences in $b$. This may duplicate $e$; but $\gradf{x}$ is a function
      that may be applied (via $\lmapply$) to many different arguments, and we want to specialise it for each
      such call.  (I think.)
    \item Optimise using the rules of $(\lmapply)$ in Figure~\ref{fig:lm-laws}.
    \item Use standard Common Subexpression Elimination (CSE) to recover any lost sharing.
  \end{itemize}
\end{itemize}

Note that
\begin{itemize}
\item The transformation is fully compositional; each function can be AD'd independently.
  For example, if a user-defined
  function~$f$ calls another user-defined function~$g$, we construct $\gradf{g}$ as
  described; and then construct $\gradf{f}$. The latter simply calls $\gradf{g}$.

\item The AD transformation is \emph{partial}; that is, it does not work for every
  program. In particular, it fails when applield to a lambda, or an application; and,
  as we will see in Section~\ref{sec:ad-vectors}, it requires that $\buildfun$ appears
  applied to a lambda.

\item We give the full Jacobian for some built-in functions in Figure~\ref{fig:ad}, including
  for conditionals ($\gradf{\iffun}$).
\end{itemize}

\subsection{Forward and reverse AD}

Consider
\begin{lstlisting}
  f(x) = p( q( r( x )))
\end{lstlisting}
Just running the algorithm above on $f$ gives
$$
\begin{array}{rcl}
  f(x)  & = & p( q( r( x ))) \\
  \gradf{f}(x) & = & \gradf{p} \lmcomp (\gradf{q} \lmcomp \gradf{r}) \\
  \fwdDf{f}(x,dx) & = & (\gradf{p} \lmcomp (\gradf{q} \lmcomp \gradf{r})) \lmapply dx \\
  & = & \gradf{p} \lmapply ((\gradf{q} \lmcomp \gradf{r}) \lmapply dx) \\
  & = & \gradf{p} \lmapply (\gradf{q} \lmapply (\gradf{r} \lmapply dx)) \\
  \gradft{f}(x) & = & (\gradft{r} \lmcomp \gradft{q}) \lmcomp \gradft{p} \\
  \revDf{f}(x,dr) & = & ((\gradft{r} \lmcomp \gradft{q}) \lmcomp \gradft{p}) \lmapply dr \\
     & = & (\gradft{r} \lmcomp \gradft{q}) \lmapply (\gradft{p} \lmapply dr) \\
     & = & \gradft{r} \lmapply (\gradft{q} \lmapply (\gradft{p} \lmapply dr)) \\
\end{array}
$$

In \emph{``The essence of automatic differentiation''} Conal says (Section 12)
\begin{quote}
The AD algorithm derived in Section 4 and generalized in Figure 6 can be thought of as a family
of algorithms. For fully right-associated compositions, it becomes forward mode AD; for fully
left-associated compositions, reverse-mode AD; and for all other associations, various mixed modes.
\end{quote}
But the forward/reverse difference shows up quite differently here: it has nothing to do
with \emph{right-vs-left association}, and everything to do with \emph{transposition}.

This is mysterious.  Conal is not usually wrong.  I would like to
understand this better.
\tom{I was also puzzled by this.  Conal's claim is suspicious to me,
  but firstly it's very cool and secondly it's Conal, so I want it to
  be true and I still hope it is.}

\tom{Now some time has passed I think I can justify why the difference
  between forward and reverse modes cannot simply be the order of
  association.  In the case of only two functions, say $f$ and $g$,
  there is no difference between the ``left'' and ``right''
  associations their composition: they are both just \(f \circ g\).
  Yet we do expect there to be a difference between the forward and
  reverse mode \emph{derivatives} of the composition.  Therefore there
  must be more to it than the order of composition.}

\section{AD for vectors} \label{sec:ad-vectors}

Like other built-in functions, each built-in function for vectors
has has its full Jacobian versions, defined in Figure~\ref{fig:built-in}.
You may enjoy checking that $\gradf{\sumfunname}$ and
$\gradf{\indexfunname}$ are correct!

For $\buildfun$ there are two possible paths, and it's not yet clear
which is best
\begin{description}
\item[Direct path.]  Figure~\ref{fig:ad} includes a rule
  for $\grad{\buildfun(e_n, \lambda i.e)}$.
\end{description}
But $\buildfun$ is an exception!  It is handled specially
by the AD transformation in Figure~\ref{fig:ad}; there is no $\gradf{\buildfun}$.
Moreover the AD transformation only works if the second argument of the build is
a lambda, thus $\buildfun(e_n, \lambda i.e)$.  I tried dealing with build and
lambdas separately, but failed (see Section~\ref{sec:build-lam-fail}).

I did think about having a specialised linear map for indexing, rather
than using $\lmhcatv$, but then I needed its transposition, so just
using $\lmhcatv$ seemed more economical.  On the other hand, with the
fucntions as I have them, I need the grotesquely delicate optimisation
rule
$$
\begin{array}{l}
\sumfun{\buildfun( n, \lambda i.\, \mbox{\lstinline|if|} \, i == e_i \,
     \mbox{\lstinline|then|} \, e \,
     \mbox{\lstinline|else|} \, 0 )} \\
\quad = \;\; \mbox{\lstinline|let|}\, i = e_i \, \mbox{\lstinline|in|}\, b \\
\mbox{if} \; i \not\in e_i
\end{array}
$$
I hate this!

\subsection{General folds} \label{sec:gen-fold}

We have $\sumfunname :: \vector{\real} \to \real$.  What is $\gradf{\sumfunname}$?
One way to define its semantics is by applying it:
$$
\begin{array}{rcl}
  \gradf{\sumfunname} & :: & \vector{\real} \to (\vector{\real} \linto \real) \\
  \gradf{\sumfunname}(v) \lmapply dv & = & \sumfun{dv}
\end{array}
$$
That is OK.  But what about product, which multiplies all the elements
of a vector together? If the vector had three elements we might have
$$
\begin{array}{l}
  \gradf{product}([x_1,x_2,x_3]) \lmapply [dx_1, dx_2, dx_3] \\
  \quad = (dx_1 * x_2 * x_3) + (dx_2 * x_1 * x_3) + (dx_3 * x_1 * x_2)
\end{array}
$$
This looks very unattractive as the number of elements grows.  Do we need
to use product?

This gives the clue that taking the derivative of $\mathit{fold}$ is
not going to be easy, maybe infeasible!  Much depends on the
particular lambda it appears.  So I have left out product, and made
no attempt to do general folds.


\section{Avoiding duplication}

\subsection{ANF and CSE}

We may want to ANF-ise before AD to avoid gratuitous duplication.
  E.g.
$$
  \begin{array}{rcl}
    \multicolumn{3}{l}{\grad{sqrt(x+(y*z))}} \\
      & = & \gradf{sqrt}(x+(y*z)) \lmcomp \grad{x+(y*z)} \\
    & = & \gradf{sqrt}(x+(y*z)) \lmcomp  \gradf{+}(x, y*z) \\
     && \lmcomp (\grad{x} \lmvcat \grad{y*z}) \\
    & = & \gradf{sqrt}(x+(y*z)) \lmcomp \gradf{+}(x, y*z) \\
    & & \lmcomp (\gradf{x} \lmvcat (\gradf{*}(y,z) \lmcomp (\gradf{y} \lmvcat \gradf{z}))) \\
  \end{array}
  $$
Note the duplication of $y*z$ in the result.
Of course, CSE may recover it.

\tom{Yes, although when I say ``AD'' I mean something that is distinct
  from what I mean by ``symbolic differentiation''.  In particular by
  ``AD'' I mean something that preserves sharing in a way that
  symbolic differentation doesn't.  Perhaps between us we should pin
  down some terminology.} \simon{I don't understand this. Perhaps you can make
  it precise?}

\tom{Consider $exp(exp(x))$.  I consider its ``symbolic derivative''
  to be $exp(exp(x)) exp(x)$ and its ``forward automatic derivative''
  to be $let~ y = exp(x)~ in~ exp(y) y$.  In other words, taking
  proper care of sharing is what makes AD AD and not just any old form
  of symbolic differentiation, in my personal nomenclature at least.
  Does that make it any clearer what I mean?}
  \awf{For me, ``AD'' very specifically implies a second argument to the function.   That's how you detect it's AD.  I.e.  $f'(x,dx) = ....$ is forward mode, and $f`(x,df) = ...$ is reverse mode.  There's a lot of chat about what AD really is, and those who want to avoid such chat often now say ``algorithmic differentiation'', to mean ``all this stuff''.  The real claim we want to explore is this: ``Forward mode is good for functions with small inputs and large outputs, e.g. $\real \mapsto \real^n$, and reverse mode is for $\real^n \mapsto \real$.}

% ----------------------------------------------

\subsection{Tupling: basic version}

\begin{figure*}
  \fbox{\begin{minipage}{\textwidth}
$$
\begin{array}{ll}
  \mbox{\bf Original function}   & f : S \to T \\
   & f(x) = e \\[2mm]
  \mbox{\bf Full Jacobian}       & \tgradf{f}  :  S \to (T, S \linto T) \\
  & \mbox{\lstinline|$\tgradf{f}(x)$ = let $\;\tgradf{x}$ = $(x, \lmone)\;$ in $\;\tgrad{e}$|} \\[2mm]
  \mbox{\bf Forward derivative}  & \fwdDf{f} : (S,\tangent{S}) \to (T,\tangent{T}) \\
   & \fwdDf{f}(x,dx) = \tgradf{f}(x) \olmapply  dx \\[2mm]
  \mbox{\bf Reverse derivative}  & \revDf{f} : (S,\tangent{T}) \to (T,\tangent{S}) \\
   & \revDf{f}(x,dfr) = dr \olmapplyR \tgradf{f}(x)
\end{array}
$$
      {\bf Differentiation of an expression}
      \begin{center}
        If $e : T$ then $\tgrad{e} : (S \linto T, T)$
      \end{center}
$$
      \begin{array}{rcll}
        \tgrad{k} & = & (k, \lmzero) \\
        \tgrad{x} & = & \tgradf{x} \\
        \tgrad{(e_1,e_2)} & = & \tgrad{e_1} \olmvcat \tgrad{e_2} \\
        \tgrad{f(e)}
        & = & \begin{array}[t]{l}
           \mbox{\lstinline|let $\;a\;$ = $\;\tgrad{e}\;$ in|} \\
           \mbox{\lstinline|let $\;r\;$ = $\;\tgradf{f}( \fst{a} )\;$ in|} \\
           (\fst{r},\; \snd{r} \lmcomp \snd{a})
           \end{array} \\
        \tgrad{\mbox{\lstinline|let $\;x$ = $e_1\;$ in $\;e_2$|}}
        & = & \mbox{\lstinline|let $\;\tgradf{x}\;$ = $\;\grad{e_1}\;$ in $\;\tgrad{e_2}$|} \\
%
%        build( n, \i.e )
%        (x) :: ((t1, s -o t1), (t2, s -o t2)) -> ((t1,t2), s -o (t1,t2))

        \tgrad{\buildfun(e_n, \lambda i.e)}
        & = & \begin{array}[t]{l}
          \mbox{\lstinline|let $\;p\;$ = $\;\swap(\buildfun(e_n, \lambda i. \tgrad{e}))\;$ in|} \\
          (\fst{p}, \; \lmvcatv{\snd{p}})
          \end{array} \\
%         \grad{\lambda i \dot e} & = & \lmlam(\lambda i\dot \grad{e}) \\
      \end{array}
      $$
\\
      {\bf Modified linear-map operations} \\
$$
 \begin{array}{rcl}
   (\olmapply) & : & (r, s \linto t) \to \tangent{s} \to \tangent{t} \\
   (v,m) \olmapply ds & = & m \lmapply ds \\[2mm]

   (\olmapplyR) & : & \tangent{t} \to (r, s \linto t) \to \tangent{s} \\
   dr \olmapplyR vm & = & dr \olmapply vm \\[2mm]

   (\olmvcat) & : & ((t_1, s \linto t_1),\; (t_2, s \linto t_2))
                    \to ((t_1,t_2), \; s \linto (t_1,t_2)) \\
    (t_1,m_1) \olmvcat (t_2,m_2) & = & ((t_1,t_2), m_1 \lmvcat m_2) \\[2mm]
   (\olmhcat) & : & ((t_1, t_1 \linto s),\; (t_2, t_2 \linto s))
                     \to ((t_1,t_2),\; (t_1,t_2) \linto s) \\
    (t_1,m_1) \olmhcat (t_2,m_2) & = & ((t_1,t_2), m_1 \lmhcat m_2) \\[2mm]
    \swap & : & \vector{(a,b)} \to (\vector{a}, \vector{b}) \\[2mm]
    \olmtrans{\cdot} & : & (r, s \linto t) \to (r, t \linto s)
%   B  & : & (N, N \to (s \linto t, t)) \to (s \linto \vector{t}, \vector{t})
 \end{array}
$$
 \\
   {\bf Derivatives of built-in functions}
$$
      \begin{array}{rcl}
        \tgradf{+}      & :: & (\real,\real) \to ((\real,\real) \linto \real, \real) \\
        \tgradf{+}(x,y) & = & (\lmone \lmhcat \lmone, \,x+y  ) \\[2mm]
        \tgradf{*}      & :: & (\real,\real) \to ((\real,\real) \linto \real, \real) \\
        \tgradf{*}(x,y) & = & (\lmscalar{y} \lmhcat \lmscalar{x}, \, x*y)  \\[2mm]
%        \gradf{\sel{1}{2}}      & :: & (t,t) \to ((t,t) \linto t) \\
%        \gradf{\sel{1}{2}}(x) & = & \lmone \lmhcat \lmzero \\[2mm]
%        \gradf{\indexfunname} & :: & (\nat, \;\vector{t}) \to ((\nat, \; \vector{t}) \linto t) \\
%        \gradf{\indexfunname}(i,v) & = & \lmzero \lmhcat \lmbuildt( \sizefun(v),
%               \lambda j. \mbox{\lstinline|if|}\, i=j \,\mbox{\lstinline|then|}\, \lmone  \, \mbox{\lstinline|else|}\, \lmzero ) \\[2mm]
%         \gradf{\sumfunname} & :: & Field \; t \darrow \vector{t} \to (\vector{t} \linto t) \\
%         \gradf{\sumfun{v}} & = & \lmbuildt( \sizefun(v),\; \lambda i. \lmone ) \\[2mm]
%        \ldots
        \end{array}
$$
\end{minipage}}
\caption{Automatic differentiation: tupling} \label{fig:ad2}
\end{figure*}

A better (and well-established) path is to modify $\gradf{f} : S \to (S \linto T)$ so that
it returns a pair:
$$
   \ogradf{f}  :  \forall a. (a \linto S, S) \to (a \linto T, T)
$$
That is $\ogradf{f}$ returns the ``normal result'' $T$ as well as a linear map.

\subsection{Polymorphic tupling: forward mode}

\begin{figure*}
  \fbox{\begin{minipage}{\textwidth}
$$
\begin{array}{ll}
  \mbox{\bf Original function}   & f : S \to T \\
  & f(x) = e \\[2mm]
  \mbox{\bf Full Jacobian}       & \ogradf{f}  :  \forall a.\, (S, a \linto S) \to (T, a \linto T) \\
  & \mbox{\lstinline|$\ogradf{f}(\oarg{x})\;$ = $\;\ograd{e}$|} \\[2mm]
  \mbox{\bf Transposed Jacobian}   & \ogradft{f}  :  \forall a.\,(S, S \linto a) \to (T, T \linto a) \\
  & \mbox{\lstinline|$\ogradft{f}(\oarg{x})\;$ =  $\;\olmtrans{(\ogradf{f}(\oarg{x}))}$|}  \\[2mm]
  \mbox{\bf Forward derivative}  & \fwdDf{f} : (S,\tangent{S}) \to (T,\tangent{T}) \\
   & \fwdDf{f}(x,dx) = \ogradf{f}(x,\lmone) \olmapply  dx \\[2mm]
  \mbox{\bf Reverse derivative}  & \revDf{f} : (S,\tangent{T}) \to (T,\tangent{S}) \\
  & \revDf{f}(x,dr) = \ogradft{f}(x,\lmone) \olmapply dr
\end{array}
$$
      {\bf Differentiation of an expression}
      \begin{center}
        If $e : T$ then $\ograd{e} : (T, a \linto T)$
      \end{center}
$$
      \begin{array}{rcll}
        \ograd{k} & = & (k, \lmzero) \\
        \ograd{x} & = & \oarg{x} \\
        \ograd{f(e)} & = & \ogradf{f}(\;\ograd{e}\;) \\
        \ograd{(e_1,e_2)} & = & \ograd{e_1} \olmvcat \ograd{e_2} \\
        \ograd{\mbox{\lstinline|let $\;x$ = $e_1\;$ in $\;e_2$|}}
        & = & \mbox{\lstinline|let $\;\oarg{x}$ = $\ograd{e_1}\;$ in $\; \ograd{e_2}$|} \\
%
%        build( n, \i.e )
%        (x) :: ((t1, s -o t1), (t2, s -o t2)) -> ((t1,t2), s -o (t1,t2))

%        \grad{\buildfun(e_n, \lambda i.e)} & = & \lmbuild(e_n, \lambda i. \grad{e}) \\
%         \grad{\lambda i \dot e} & = & \lmlam(\lambda i\dot \grad{e}) \\
      \end{array}
      $$
\\
      {\bf Modified linear-map operations} \\
$$
 \begin{array}{rcl}
   (\olmapply) & : & (r, s \linto t) \to \tangent{s} \to (r, \tangent{t}) \\
   (v,m) \olmapply ds & = & (v, m \lmapply ds) \\[2mm]
   (\olmvcat) & : & ((t_1, s \linto t_1),\; (t_2, s \linto t_2)) \to ((t_1,t_2),\; s \linto (t_1,t_2)) \\
    (t_1,m_1) \olmvcat (t_2,m_2) & = & ((t_1,t_2), m_1 \lmvcat m_2) \\[2mm]
   (\olmhcat) & : & ((t_1, t_1 \linto s),\; (t_2, t_2 \linto s)) \to ((t_1,t_2),\; (t_1,t_2) \linto s) \\
    (t_1,m_1) \olmhcat (t_2,m_2) & = & (t_1 + t_2, m_1 \lmhcat m_2) \\[2mm]
    \olmtrans{\cdot} & : & (t, s \linto t) \to (t, t \linto s)
%   B  & : & (N, N \to (s \linto t, t)) \to (s \linto \vector{t}, \vector{t})
 \end{array}
$$
 \\
   {\bf Derivatives of built-in functions}
$$
      \begin{array}{rcl}
        \ogradf{+}      & :: & \forall a. ((\real,\real), a \linto (\real,\real)) \to (\real, a \linto \real) \\
        \ogradf{+}((x,y),m) & = & (x+y,\,(\lmone \lmhcat \lmone) \lmcomp m ) \\[2mm]
        \ogradf{*}      & :: & \forall a. ((\real,\real), a \linto (\real,\real)) \to (\real, a \linto \real) \\
        \ogradf{*}((x,y), m) & = & (x*y,\, (\lmscalar{y} \lmhcat \lmscalar{x}) \lmcomp m)  \\[2mm]
%        \gradf{\sel{1}{2}}      & :: & (t,t) \to ((t,t) \linto t) \\
%        \gradf{\sel{1}{2}}(x) & = & \lmone \lmhcat \lmzero \\[2mm]
%        \gradf{\indexfunname} & :: & (\nat, \;\vector{t}) \to ((\nat, \; \vector{t}) \linto t) \\
%        \gradf{\indexfunname}(i,v) & = & \lmzero \lmhcat \lmbuildt( \sizefun(v),
%               \lambda j. \mbox{\lstinline|if|}\, i=j \,\mbox{\lstinline|then|}\, \lmone  \, \mbox{\lstinline|else|}\, \lmzero ) \\[2mm]
%         \gradf{\sumfunname} & :: & Field \; t \darrow \vector{t} \to (\vector{t} \linto t) \\
%         \gradf{\sumfun{v}} & = & \lmbuildt( \sizefun(v),\; \lambda i. \lmone ) \\[2mm]
%        \ldots
        \end{array}
$$
\end{minipage}}
\caption{Automatic differentiation: polymorphic tuples} \label{fig:ad2}
\end{figure*}

Everything works much more compositionally if $\ogradf{f}$ also
\emph{takes} a linear map as its input.  The new transform is shown in
Figure~\ref{fig:ad2}. Note that there is no longer any code
duplications, even without ANF or CSE.

In exchange, though, all the types are a bit more complicated.  So we
regard Figure~\ref{fig:ad} as canonical, to be used when working
thiungs out, and Figure~\ref{fig:ad2} as a (crucial) implementation
strategy.

The crucial property are these:
$$
\begin{array}{rrcl}
(CP) & \ogradf{f}(e) \olmapply dx & = & \fwdDf{f}(e \olmapply dx) \\
\end{array}
$$
Crucial because suppose we have
$$
\lstinline|f(x) = g(h(x))|
$$
Then, we can transform as follows, using (CP)  twice, on lines marked $(\dagger)$:
$$
\begin{array}{rcll}
  \ogradf{f}(\oarg{x}) & = & \ogradf{g}(\;\ogradf{h}(\;\oarg{x}\;)\;) \\
  \fwdDf{f}(x, dx) & = & \ogradf{g}(\;\ogradf{h}(\;x,\lmone\;)\;) \olmapply dx \\
                   & = & \fwdDf{g}( \;\ogradf{h}(\;x,\lmone\;) \olmapply dx \;) & (\dagger)\\
                   & = & \fwdDf{g}(\; \fwdDf{h}(\;(x,\lmone) \olmapply dx\; ))  & (\dagger) \\
                   & = & \fwdDf{g}(\; \fwdDf{h}(\;x,\lmone \lmapply dx\; ))   \\
                   & = & \fwdDf{g}(\; \fwdDf{h}(\;x, dx\; )) \\
\end{array}
$$
Why is (CP) true?   It follows from a more general property of $\ogradf{f}$:
$$
\begin{array}{l}
  \forall f \tcolon S \to T,\, x \tcolon S,\, m_1 \tcolon A \linto S,\, m_2 \tcolon B \linto A,\, db\tcolon \tangent{B}. \\
 \quad    \ogradf{f}(x,m_1) \olmapply (m_2 \lmapply db) = \ogradf{f}(x, m_1 \lmcomp m_2) \olmapply db \\[2mm]

  \forall f \tcolon S \to T,\, x \tcolon S,\, m_1 \tcolon S \linto A,\, m_2 \tcolon A \linto B,\, dr\tcolon \tangent{T}. \\
 \quad    m_2 \lmapply (\ogradft{f}(x,m_1) \olmapply dr) = \ogradft{f}(x, m_2 \lmcomp m_1) \olmapply dr
\end{array}
$$
Now we can prove our claim as follows
$$
\begin{array}{lll}
\multicolumn{3}{l}{\fwdDf{f}(e \olmapply dx)} \\
& = & \mbox{ \{by defn of $(\olmapply)$\} } \\
& & \fwdDf{f}( \fst{e},\; \snd{e} \lmapply dx ) \\[1mm]
& = & \mbox{ \{by defn of $\fwdDf{f}$\}} \\
& & \ogradf{f}(\fst{e} ,\; \lmone) \olmapply (\snd{e} \lmapply dx) \\[1mm]
& = & \mbox{ \{by crucial property\}} \\
& & \ogradf{f}(\fst{e} ,\; \snd{e} ) \olmapply dx \\[1mm]
& = & \ogradf{f}(e) \olmapply dx
\end{array}
$$

\subsection{Polymorphic tupling: reverse mode}

It turns out that things work quite differently for reverse mode.
For a start the equivalent of (CP) for reverse-mode would look like this:
$$
  \ogradft{f}(e) \olmapply dr  =  \revDf{f}(e \olmapply dr)
$$
But this is not even well-typed!

How did we use (CP)?  Supppose $f$ is defined in terms of $g$ and $h$:
$$
\lstinline|f(x) = g(h(x))|
$$
Then we want $\fwdDf{f}$ to be defined in terms of $\fwdDf{g}$ and $\fwdDf{h}$.
That is, we want a \emph{compositional} method, where we can create the code for
$\fwdDf{f}$ without looking at the code for $g$ or $h$, simpply by calling
$g$ and $h$'s derived functions. And that's just what we achieved:
$$
\fwdDf{f}(x,dx) = \fwdDf{g}(\fwdDf{h}(x,dx))
$$

But for reverse mode, this plan is much less straightforward.   Look at the types:
$$
\begin{array}{rcl}
f & : & R \to T \\
g & : & S \to T \\
h & : & R \to S \\
\revDf{f} & : & (R, \tangent{T}) \to (T, \tangent{R}) \\
\revDf{g} & : & (S, \tangent{T}) \to (T, \tangent{S}) \\
\revDf{h} & : & (R, \tangent{S}) \to (S, \tangent{R}) \\
\end{array}
$$
How can we define $\revDf{f}$ by calling $\revDf{g}$ and $\revDf{h}$?  It would
have to look something like this
$$
\begin{array}{rcl}
\revDf{f}(r,dt) & = & \lstinline|letrec| \begin{array}[t]{l}
                          (t, ds) = \revDf{g}( s, dt ) \\
                          (s, dr) = \revDf{h}( r, ds ) \\
                        \end{array} \\
 && \lstinline|in| \; (t, dr)
\end{array}
$$
We can't call $\revDf{g}$ before $\revDf{h}$, nor the other way around.
That's why there is a letrec!  Even leaving aside how we generate this code,
We'd need lazy evaluation to execute it.

The obvious alternative is to change $\fwdDf{f}$'s interface.  Currently we have
$$
\revDf{f} : (R, \tangent{T}) \to (T, \tangent{R}) \\
$$
Instead, we can take that $R$ value, but return a function
$\tangent{T} \to \tangent{R}$, thus:
$$
\revDf{f} : R \to (T, \tangent{T} \to \tangent{R})
$$
But that commits to returning a \emph{function}, with its fixed, built-in representation.
Instead, let's return linear map:
$$
\revDf{f} : R \to (T, \tangent{T} \linto \tangent{R})
$$
Now we can re-interpret the retuned linear map as some kind of record (trace) of
all the things that $f$ did.  And if we insist on our compositional account
we really must \emph{manifest} that data structure, and later apply it to a value of
type $\tangent{T}$ to get a value of type $\tangent{R}$.  We could represent those linear maps as:
\begin{itemize}
\item A matrix
\item A function closure that, when called, applies the linear map to an argument
\item A syntax tree whose nodes are the constructors of the linear map type.
  When applying the linear map, we interpret taht syntax tree.
\end{itemize}
Finally, notice that this final version of $\fwdDf{f}$ is exactly $\ogradft{f}$,
just specialised with an input linear map of $\lmone$.  So we may as well
just use $\ogradft{f}$, which \emph{already} compositionally calls $\ogradft{g}$
and $\ogradft{h}$.

TL;DR: for reverse mode, we must simply compile $\ogradft{f}$.

Notice that we can get quite a bit of optimisation by inlining $\ogradft{g}$ into $\ogradft{f}$,
and so on. The more inlining the better.  If we inline everything we'll elminate
all intermediate linear maps.


\section{Compiling through categories}


\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
$$
      \begin{array}{rcll}
        \multicolumn{4}{l}{\mbox{\bf Atoms}} \\
        f,g,h & ::= & \multicolumn{2}{l}{\mbox{Function}} \\
        k & ::= & \multicolumn{2}{l}{\mbox{Literal constants}} \\
        \multicolumn{4}{l}{\mbox{\bf Terms}} \\
        \mathit{pgm} & ::= & \mathit{def}_1 \ldots \mathit{def}_n \\
        \mathit{def} & ::= & f {:} S \darrow T = c \\
        c & ::= & \cid & \mbox{Identity} \\
          & |   & \ck{k} & \mbox{Constant} \\
          & |   & \cprune{i_1,\ldots,i_m}{n}& \mbox{Pruning} (0 \leq m \leq n) \\
          & |   & \cfun{f} & \mbox{Function constant} \\
          & |   & c_1 \clcomp c_2  & \mbox{Composition} \\
          & |   & (c_1,\ldots, c_n) & \mbox{Tuple} \\
          & |   & \cif{c_1}{c_2}{c_3} & \mbox{Conditional} \\
          & |   & \clet{x}{c_r}{c_b} & \mbox{Let} \\
          & |   & \cbuild{c_s}{i}{c_e} & \mbox{Build}
      \end{array}
$$
\end{minipage}}
\caption{Syntax of \CatLang} \label{fig:cl-syntax}
\end{figure}

% --------------------- Semantics
\begin{figure}
\fbox{\begin{minipage}{\columnwidth}
\mbox{{\bf Semantics (aka conversion from \CatLang):}\; \fbox{$e \capply c = e$}}
$$
\begin{array}{r@{\hspace{1mm}}c@{\hspace{1mm}}l}
   t \capply \cid & = & t \\
   t \capply \cprune{i_1,\ldots,i_m}{n} & = & (\sel{i_1}{n}(t), \ldots, \sel{i_m}{n}(t)) \\
   t \capply \ck{k} & = & k \\
   t \capply \cfun{f} & = & f(t) \\
   t \capply (c_1 \clcomp c_2)  & = & (t \capply c_1) \capply c_2 \\
   t \capply (c_1,\ldots, c_n)  & = & (t \capply c_1,\ldots, t \capply c_n) \\
   t \capply \cif{c_1}{c_2}{c_3} & = & \lstinline|if|\; (t \capply c_1) \; (t \capply c_2)\; (t \capply c_3) \\
   t \capply \clet{x}{c_r}{c_b} & = & \lstinline|let|\;x\, = \, t \capply c_r \; \lstinline{in}\;  (t \ctupx x) \capply c_b \\
   t \capply \cbuild{c_s}{i}{c_e} & = & \lstinline|build|\;(t \capply c_x)\;(\lambda i. (t \ctupx i) \capply c_e)
\end{array}
$$
\mbox{\bf Conversion to \CatLang}
$$
\begin{array}{l}
\begin{array}{rcl}
\Gamma & ::= & (x_1\tcolon\tau_1, \ldots, x_n\tcolon\tau_n) \\[1mm]
\end{array} \\
\begin{array}{rcl}
\phi((x_1\tcolon\tau_1,\ldots, x_n\tcolon\tau_n),\,x_i) & = & i \\
T(x_1\tcolon\tau_1, \ldots, x_n\tcolon\tau_n) & = & (\tau_1, \ldots, \tau_n) \\
\end{array}
\\[5mm]
\begin{array}{l}
  \KC{f(x_1\tcolon\tau_1,\ldots,x_n\tcolon\tau_n) = e}{} \\
  \quad = \quad \cfun{f} = \KC{e}(x_1\tcolon\tau_1,\ldots,x_n\tcolon\tau_n)
  \end{array} \\ \\
\begin{array}{r@{\hspace{1mm}}c@{\hspace{1mm}}l}
\multicolumn{3}{c}{\fbox{If $\Gamma \vdash e:\tau$ then $\KC{e}\Gamma : T(\Gamma) \darrow \tau$}} \\
  \KC{k}{\Gamma} & = & \ck{k} \\
  \KC{x}{\Gamma} & = & \cfun{\pi(\Gamma,x)} \\
  \KC{f(e)}{\Gamma} & = & \KC{e}{\Gamma} \clcomp \cfun{f} \\
  \KC{\lstinline|if|\;e_1\;e_2\;e_3}{\Gamma} \\
     \multicolumn{3}{l}{\hspace{2cm} = \cif{\KC{e_1}{\Gamma}}{\KC{e_2}{\Gamma}}{\KC{e_3}{\Gamma}}} \\
  \KC{(e_1, \ldots, e_n)}{\Gamma} & = & (\KC{e_1}{\Gamma}, \ldots, \KC{e_n}{\Gamma}) \\
  \KC{\lstinline|let|\;x{:}\tau = e_r \;\lstinline|in|\; e_b}{\Gamma} & = & \clet{x}{\KC{e_r}{\Gamma}}{\KC{e_b}{(\Gamma,x{:}\tau)}} \\
  \KC{\lstinline|build|\;e_s\;(\lambda i.e_e)}{\Gamma} & = & \cbuild{\KC{e_s}{\Gamma}}{i}{\KC{e_e}{(\Gamma,i)}}
\end{array}
\\[1mm]
\mbox{\bf Pruning} \\
\begin{array}{r@{\hspace{1mm}}c@{\hspace{1mm}}l}
  \KC{e}\Gamma & = & \cprune{\phi(\Gamma,v_1), \ldots, \phi(\Gamma, v_m)}{\sizefun(\Gamma)}(\KC{e}{\Gamma'}) \\
  \multicolumn{3}{l}{\quad \mbox{where}
    $\begin{array}[t]{r@{\hspace{1mm}}c@{\hspace{1mm}}l}
        \{v_1, \ldots, v_m\} & = & \mathit{fv}(e) \\
        \Gamma' & = & (v_1\tcolon\Gamma(v_1), \ldots, v_n\tcolon\Gamma(v_n))
      \end{array}$} \\
\end{array}
\end{array}
$$
\end{minipage}}
\caption{Semantics of \CatLang} \label{fig:cl-semantics}
\end{figure}

\begin{figure}
  \fbox{\begin{minipage}{\columnwidth}
  \begin{center}{\large \fbox{$\Gamma \vdash c : S \darrow T$}}\end{center}

  \begin{gather*}
    \infer{\Gamma \vdash \cid : S \darrow S}
          {} \qquad
    \\[1mm]
    \infer{\Gamma \vdash \cprune{i_1, \ldots, i_m}{n} : (s_1, \ldots, s_n) \darrow (s_{i_1}, \ldots, s_{i_m})}
          {}
   \\[1mm]
    \infer{\Gamma \vdash \cfun{f} : S \darrow T}
          {f : S \to T \in \Gamma} \qquad
    \infer{\Gamma \vdash \ck{k} : () \darrow \real}
          {}
   \\[1mm]
   \infer{\Gamma \vdash c_1 \clcomp c_2 : S \darrow T}
   {\Gamma \vdash c_1 : S \darrow R \quad \Gamma \vdash c_2 : R \darrow T}
   \\[1mm]
   \infer{\Gamma \vdash (c_1, \ldots, c_n) : S \darrow (T_1, \ldots, T_n)}
    {\Gamma \vdash c_1:S \darrow T_1 \quad \ldots \quad \Gamma \vdash c_n:S \darrow T_n}
   \\[1mm]
   \infer{\Gamma \vdash \cif{c_1}{c_2}{c_3} : S \darrow T}
    {\Gamma \vdash c_1 : S \darrow \bool \quad \Gamma \vdash c_2:S \darrow T \quad  \Gamma \vdash c_3:S \darrow T}
   \\[1mm]
   \infer{\Gamma \vdash \clet{x}{c_r}{c_b} : S \darrow T}
    {\Gamma \vdash c_r : S \darrow R \quad \Gamma \vdash c_b: (S \ctupxt R) \darrow T}
   \\[1mm]
   \infer{\Gamma \vdash \cbuild{c_s}{i}{c_e} : S \darrow \vector{T}}
    {\Gamma \vdash c_s : S \darrow \nat \quad \Gamma \vdash c_e: (S \ctupxt \nat) \darrow T}
  \end{gather*}
\end{minipage}}
  \caption{Type system for \CatLang} \label{fig:cl-types}
\end{figure}

\subsection{Splitting for reverse mode}

Supppose $f$ is defined in terms of $g$ and $h$:
$$
\lstinline|f(x) = g(h(x))|
$$
Here are the types:
$$
\begin{array}{rcl}
f & : & R \to T \\
g & : & S \to T \\
h & : & R \to S \\
\revDf{f} & : & (R, \tangent{T}) \to (T, \tangent{R}) \\
\revDf{g} & : & (S, \tangent{T}) \to (T, \tangent{S}) \\
\revDf{h} & : & (R, \tangent{S}) \to (S, \tangent{R}) \\
\end{array}
$$
How can we define $\revDf{f}$ by calling $\revDf{g}$ and $\revDf{h}$?  It would
have to look something like this
$$
\begin{array}{rcl}
\revDf{f}(r,dt) & = & \lstinline|letrec| \begin{array}[t]{l}
                          (t, ds) = \revDf{g}( s, dt ) \\
                          (s, dr) = \revDf{h}( r, ds ) \\
                        \end{array} \\
 && \lstinline|in| \; (t, dr)
\end{array}
$$
We can't call $\revDf{g}$ before $\revDf{h}$, nor the other way around.
That's why there is a letrec!  Even leaving aside how we generate this code,
We'd need lazy evaluation to execute it.

The key idea for splitting is this.  Given $f : S \to T$, produce two functions
$$
\begin{array}{rcl}
  \revfDf{f} & : & S \to (T,X) \\
  \revrDf{f} & : & (X,\tangent{T}) \to \tangent{S}
\end{array}
$$
where the type $X$ depends on the details of f's definition.  The idea is that
$X$ records all the stuff that $f$ computed when running forward that is necessary
for it to run backward.  Now we can write
$$
\begin{array}{rcl}
\revDf{f}(s,dt) & = & \lstinline|letrec| \begin{array}[t]{l}
                          (t, \mathit{xf}) = \revfDf{f}( s ) \\
                          ds = \revfDf{f}( \mathit{xf}, dt ) \\
                        \end{array} \\
&& \lstinline|in| \; (t, ds)
\\[1mm]

\revfDf{f}(r) & = & \lstinline|letrec| \begin{array}[t]{l}
                          (s, xh) = \revfDf{h}( r ) \\
                          (t, xg) = \revfDf{g}( r ) \\
                        \end{array} \\
 && \lstinline|in| \; (t, (xh,xg))
\\[1mm]
\revrDf{f}((xh,xg),dt) & = & \revrDf{h}(dh, \revrDf{g}(dg, gt))
\end{array}
$$

\section{Implementation}

The implementation differs from this document as follows:
\begin{itemize}
\item Rather than pairs, the implementation supports $n$-ary tuples.
  Similary the linear maps $(\lmvcat)$ and $\lmhcat$ are $n$-ary.
\item Functions definitions can take $n$ arguments, thus
  \begin{lstlisting}
   f(x,y,z) = e
  \end{lstlisting}
  This is treated as equivalent to
  \begin{lstlisting}
    f(t) = let x = $\sel{1}{3}$(t)
               y = $\sel{2}{3}$(t)
               z = $\sel{3}{3}$(t)
           in e
  \end{lstlisting}
\end{itemize}

\section{Fold}

\begin{figure*}
  \fbox{\begin{minipage}{\textwidth}
      {\bf Typing rules for fold} \\
\[
 \begin{array}{rcl}
   t & : & (a, b) \\
   e & : & a \\
   acc & : & a \\
   v & : & \mathrm{Vec} \,\, b \\
   \mathrm{fold} \,\, (\lambda t . e) \,\, acc \,\, v & : & a
 \end{array}
\]
      {\bf Typing rules for lmFold} \\
\[
 \begin{array}{rcl}
   t & : & (a, b) \\
   e & : & a \\
   e' & : & (s, (a, b)) \linto a \\
   acc & : & a \\
   v & : & \mathrm{Vec} \,\, b \\
   \mathrm{lmFold} \,\, (\lambda t . e) \,\, (\lambda t . e') \,\, acc \,\, v & : &
       (s, (a, \mathrm{Vec} \,\, b)) \linto a
 \end{array}
 \]
      {\bf Typing rules for FFold and RFold} \\
\[
 \begin{array}{rcl}
   t & : & (a, b) \\
   t_{dr} & : & ((a, b), \tangent a) \\
   t_{dt} & : & ((a, b), (\tangent a, \tangent b)) \\
   e & : & a \\
   e_{dr} & : & (\tangent s, (\tangent a, \tangent b)) \\
   e_{dt} & : & \tangent a \\
   acc & : & a \\
   v & : & \mathrm{Vec} \,\, b \\
   dr & : & \tangent a \\
   d_{acc} & : & \tangent a \\
   d_v & : & \mathrm{Vec} \,\, \tangent b \\
   \mathrm{FFold} \,\, (\lambda t . e) \,\, acc \,\, v \,\,
                       (\lambda t_{dt} . e_{dt}) \,\, d_{acc} \,\, d_v & : &
       \tangent a \\
   \mathrm{RFold} \,\, (\lambda t . e) \,\, (\lambda t_{dr} . e_{dr}) \,\, acc \,\, v \,\, dr & : &
       (\tangent s, (\tangent a, \mathrm{Vec} \,\, \tangent b)) \\
 \end{array}
\]
  \end{minipage}}
  \caption{Rules for fold}
\end{figure*}

\begin{figure*}
  \fbox{\begin{minipage}{\textwidth}
      {\bf Differentiation of fold}

      \newcommand{\gradS}[2]{\nabla_{#1}\lb #2 \rb}
      \newcommand{\gradV}[1]{\nabla#1}

      \begin{center}
        If $e : T$ then $\gradS{s}{e} : s \linto T$
      \end{center}
\[
      \begin{array}{rcll}
        \gradS{s}{\mathrm{fold} \,\, (\lambda t . e) \,\, acc \,\, v} & = &
        \mathrm{lmFold} \,\, (\lambda t . e) \,\, (\lambda t . e') \,\, acc \,\, v \lmcomp p\\
        \textrm{where} \,\, p & : & s \linto (s, (a, \mathrm{Vec} \,\, b)) \\
        p & = & \lmone_s \lmvcat (\gradS{s}{acc} \lmvcat \gradS{s}{v}) \\
        e' & = & \textrm{let} \,\, \gradV{x} = \gradV{x} \lmcomp (\lmone_s \lmhcat \lmzero_s^{(a,b)}) \\
        & & \ldots \,\, \textrm{for each \(x\) ocurring free in \(\lambda t.e\)} \\
        & & \textrm{let} \,\, \gradV{t} = \lmzero_{(a,b)}^s \lmhcat \lmone_{(a,b)} \\
        & & \textrm{in} \,\, \gradS{(s, (a, b))}{e} \\
      \end{array}
      \]
\\
      {\bf Applying an lmFold}
\[
      \begin{array}{rcll}
        \mathrm{lmFold} \,\, (\lambda t . e) \,\, (\lambda t . e') \,\, acc \,\, v \lmapply dx
        & =
        & \mathrm{FFold} \,\, (\lambda t . e) \,\, acc \,\, v \,\,
              (\lambda t_{dt} . e_{dt}) \,\, d_{acc} \,\, d_v
        \\
        \textrm{where} \,\, e_{dt} & = & \textrm{let} \,\, t = \fst{t_{dt}} \\
        & & \textrm{let} \,\, dt = \snd{t_{dt}} \\
        & & \textrm{in} \,\, e' \lmapply (ds, dt) \\
        ds & = & \fst{dx} \\
        d_{acc} & = & \fst{\snd{dx}} \\
        d_{v} & = & \snd{\snd{dx}} \\
        \\
        dx \lmapplyR \mathrm{lmFold} \,\, (\lambda t . e) \,\, (\lambda t . e') \,\, acc \,\, v
        & =
        & \mathrm{RFold} \,\, (\lambda t . e) \,\, (\lambda t_{dr} . e_{dr}) \,\, acc \,\, v \,\, dx
        \\
        \textrm{where} \,\, e_{dr} & = & \textrm{let} \,\, t = \fst{t_{dr}} \\
        & & \textrm{let} \,\, dr = \snd{t_{dr}} \\
        & & \textrm{in} \,\, dr \lmapplyR e' \\
      \end{array}
      \]
\end{minipage}}
\caption{Rules for fold}
\end{figure*}

\begin{figure*}
\begin{verbatim}
def FFold dA ((f  : F)  (acc  : A)  (v  : Vec n B)
              (f_ : F_) (dacc : dA) (dv : Vec n dB))
  = FFold_recursive(0, f, acc, v f_, dacc, dv)

def FFold_recursive dA ((i : Integer) (f  : F)  (acc  : A)  (v  : Vec n B)
                                      (f_ : F_) (dacc : dA) (dv : Vec n dB))
  = if i == n
    then dacc
    else let fwd_f = f_((acc, v[i]), (dacc, dv[i]))
         in FFold_recursive(i + 1, f, f(acc, v[i]), v, f_, fwd_f, dv)
\end{verbatim}
\caption{Forward mode derivative for fold}
\end{figure*}

\begin{figure*}
\begin{verbatim}
def RFold (S, (dA, Vec n dB)) ((f : F) (f_ : F_) (acc : A) (v : Vec n B) (dr : dA))
  = let (ds, dv, da) = RFold_recursive(f, f_, 0, v, acc, dr)
    in (s, (da, dv))

def RFold_recursive (S, Vec n dB, dA) ((f : F) (f_ : F_) (i : Integer) (v : Vec n B)
                                       (acc : A) (dr : dA))
  = if i == n
    then (0, 0, dr)
    else let (r_ds, r_dv, r_dacc) = RFold_recursive(f, f_, i + 1, v, f(acc, v[i]), dr)
             (f_ds, (f_dacc, f_db)) = f_((acc, v[i]), r_dacc)
         in (r_ds + f_ds, r_dv + deltaVec(i, f_db), f_dacc)
\end{verbatim}
\caption{Reverse mode derivative for fold}
\end{figure*}

\newcommand{\proctype}[3]{#2 \vdash #1 \dashv #3}

\section{Looping in split mode}

\begin{figure*}
  \fbox{\begin{minipage}{\textwidth}
      {\bf nTimes} \\
\[
 \begin{array}{rcl}
   n & : & Integer \\
   sInitial & : & s \\
   f & : & s \to s \textrm{ (known function)}\\
   \mathrm{nTimes} \,\, n \,\, sInitial \,\, f & : & s
 \end{array}
\]
      {\bf revf\$nTimes} \\
\[
 \begin{array}{rcl}
   n & : & Integer \\
   sInitial & : & s \\
   f & : & s \to s \textrm{ (known function)} \\
   \mathrm{revf\$nTimes} \,\, n  \,\, sInitial \,\, f & : &
   (s, \vector{BOG[f]})
 \end{array}
 \]
      {\bf revr\$nTimes} \\
\[
 \begin{array}{rcl}
   dds' & : & \tangent{s} \\
   bog' & : & \vector{BOG[f]} \\
   \mathrm{revr\$nTimes} \,\, (bog', dds') & : & ((), \tangent{s}, ())
 \end{array}
\]
  \end{minipage}}
  \caption{Typing rules for nTimes}
\end{figure*}

\begin{figure*}
  \begin{minipage}{\textwidth}
      {\bf Semantics of nTimes}

      \newcommand{\gradS}[2]{\nabla_{#1}\lb #2 \rb}
      \newcommand{\gradV}[1]{\nabla#1}

\[
      \begin{array}{rcll}
        \mathrm{nTimes} \,\, 0 \,\, sInitial \,\, f & = & sInitial \\
        \mathrm{nTimes} \,\, n \,\, sInitial \,\, f & = &
        \mathrm{nTimes} \,\, (n-1) \,\, f(sInitial) \,\, f & \textrm{if
          $n > 0$}\\
        \mathrm{nTimes} \,\, n \,\, sInitial \,\, f & = &
        \mathrm{undefined} & \textrm{if $n < 0$}\\
      \end{array}
      \]
\\
\begin{minipage}{\textwidth}
      {\bf revf\$nTimes}
\begin{verbatim}

revf$nTimes n s f =
  let (_, bog', s') = nTimes n (0, uninitializedVector n, s) (\(i, bog, s) ->
                          let (s', bogf) = revf$f s
                              bog'       = setAt i bog bogf
                              i'         = i + 1
                          in (i', bog', s'))
  in (bog', s')

\end{verbatim}
\end{minipage}
\\
\begin{minipage}{\textwidth}
      {\bf revr\$nTimes}
\begin{verbatim}

revr$nTimes (bog', dds') =
  let (_, _, dds) = nTimes (size bog') (size bog', bog', dds')  n (\(i', bog', dds') ->
                        let i    = i' - 1
                            bogf = index i bog'
                            bog  = bog'
                            dds  = revr$f(bogf, dds')
                        in (i, bog, dds)
  in ((), dds, ())

\end{verbatim}
\end{minipage}
\end{minipage}
\caption{Behaviour of nTimes}
\end{figure*}

\begin{figure*}
  \fbox{\begin{minipage}{\textwidth}
      {\bf ccl} \\
\[
 \begin{array}{rcl}
   n & : & Integer \\
   sInitial & : & s \\
   f & : & (Integer, s) \to s \textrm{ (known function)}\\
   \mathrm{ccl} \,\, n \,\, sInitial \,\, f & : & s
 \end{array}
\]
      {\bf revf\$ccl} \\
\[
 \begin{array}{rcl}
   n & : & Integer \\
   sInitial & : & s \\
   f & : & (Integer, s) \to s \textrm{ (known function)} \\
   \mathrm{revf\$ccl} \,\, n  \,\, sInitial \,\, f & : & (s, (n, s))
 \end{array}
 \]
      {\bf revr\$ccl} \\
\[
 \begin{array}{rcl}
   n    & : & Integer \\
   s    & : & s \\
   dds' & : & \tangent{s} \\
   \mathrm{revr\$ccl} \,\, ((n, s), dds') & : & ((), \tangent{s}, ())
 \end{array}
\]
  \end{minipage}}
  \caption{Typing rules for ccl}
\end{figure*}

\begin{figure*}
  \begin{minipage}{\textwidth}
      {\bf Semantics of ccl}

      \newcommand{\gradS}[2]{\nabla_{#1}\lb #2 \rb}
      \newcommand{\gradV}[1]{\nabla#1}

\[
      \begin{array}{rcll}
        \mathrm{ccl} \,\, 0 \,\, sInitial \,\, f & = & sInitial \\
        \mathrm{ccl} \,\, n \,\, sInitial \,\, f & = &
        \mathrm{ccl} \,\, (n-1) \,\, f(n-1, sInitial) \,\, f & \textrm{if
          $n > 0$}\\
        \mathrm{ccl} \,\, n \,\, sInitial \,\, f & = &
        \mathrm{undefined} & \textrm{if $n < 0$}\\
      \end{array}
      \]
\\
\begin{minipage}{\textwidth}
      {\bf revf\$ccl}
\begin{verbatim}

revf$ccl n s f = (ccl n s f, (n, s))

\end{verbatim}
\end{minipage}
\\
\begin{minipage}{\textwidth}
      {\bf revr\$ccl}
\begin{verbatim}

revr$ccl ((n, s), dds') =
  let (_, dds) = ccl n (s, dds') (\(i, (s, dds')) ->
                        let (s', bogf) = revf$f s
                            dds        = revr$f(bogf, dds')
                        in (s', dds)
  in ((), dds, ())

\end{verbatim}

This form of reverse pass is only correct if $revr\$f$ is
commutative in the sense that

\[
\forall
bog1, bog2, dds' \,\,
revr\$f(bog1, revr\$f(bog2, dds'))
= revr\$f(bog2, revr\$f(bog1, dds'))
\]
\end{minipage}
\end{minipage}
\caption{Behaviour of ccl}
\end{figure*}

\section{Procedure language}

The typing judgement for a procedure $p$ is of the form
$\proctype{p}{\Gamma}{\Delta}$.

Here the ``context'' $\Gamma$ contains the types of the free
variables, those which the procedure uses on the right hand side of an
$=$ sign but that were not bound on the left hand side earlier in the
procedure.  The ``co-context'' $\Delta$ contains the types of the
``cofree'' variables, i.e. those which are bound on the left hand side
of an $=$ sign but that are not used on the right hand side later in
the procedure.

The requirement that there are no compound expressions in the
procedure language leads to very verbose code, analogous to ANF.  The
reverse mode AD pass relies critically on this property, so although
we could relax the condition and permit nested subexpressions we would
have to apply an explicit ANF pass before applying the reverse mode
transform.  We will live with the verbosity for now.


\begin{figure*}
  \fbox{\begin{minipage}{\textwidth}
$$
      \begin{array}{rcll}
        \multicolumn{4}{l}{\mbox{\bf Atoms}} \\
        f,g,h & ::= & \multicolumn{2}{l}{\mbox{Function}} \\
        x,y,z & ::= & \multicolumn{2}{l}{\mbox{Variable}} \\
        k & ::= & \multicolumn{2}{l}{\mbox{Literal constant}} \\
        \\
        \multicolumn{4}{l}{\mbox{\bf Terms}} \\
        \mathit{statement} & ::= & x = Call \; f \; y & \mbox{Function call}\\
          & |   & x = Const \; k & \mbox{Constant} \\
          & |   & Elim \; y & \mbox{Elimination} \\
          & |   & x = Dup \; y & \mbox{Duplication} \\
          & |   & x = (y, z) & \mbox{Tuple constructor} \\
          & |   & (x, y) = z & \mbox{Tuple pattern match} \\
          & |   & x = Inl \; y & \mbox{Sum constructor}\\
          & |   & x = Inr \; y & \mbox{Sum constructor}\\
          & |   & Case \; x \; (Inl \; x \; procedure) (Inr \; y \; procedure)
        & \mbox{Sum pattern match}\\
        \\
        \mathit{procedure} & ::= & statement \\
          & |   & procedure \; ; \; procedure \\
        \\
        \multicolumn{4}{l}{\mbox{\bf Types}} \\
        \tau & ::= & \real & \mbox{Real numbers} \\
        & | & (\tau_1, \tau_2) & \mbox{Pairs} \\
        & | & \tau_1 \oplus \tau_2 & \mbox{Sums} \\
      \end{array}
 $$
\end{minipage}}
\caption{Syntax of the procedure language}
\end{figure*}

\begin{figure*}
  \fbox{\begin{minipage}{\textwidth}
  \begin{center}{\large \fbox{$\proctype{p}{\Gamma}{\Delta}$}}\end{center}

  \begin{gather*}
    \infer{\proctype{b = Call \; f \; a}{a : S}{b : T}}
          {f : S \to T}
   \\[1mm]
    \infer{\proctype{t = Dup \; s}{a : S}{t : (S, S)}}
          {} \qquad
   \\[1mm]
   \infer{\proctype{a = Const \; k}{}{a : T}}
         {k : T}
   \\[1mm]
   \infer{\proctype{Elim \; a}{a : T}{}}
         {}
   \\[1mm]
   \infer{\proctype{b = (a_1, a_2)}{a_1 : T_1, a_2 : T_2}{b : (T_1, T_2)}}
         {}
   \\[1mm]
   \infer{\proctype{(a_1, a_2) = b}{b : (T_1, T_2)}{a_1 : T_1, a_2 : T_2}}
         {}
   \\[1mm]
   \infer{\proctype{a = Inl \; b}{b : T_1}{a : T_1 \oplus T_2}}
         {}
   \\[1mm]
   \infer{\proctype{a = Inr \; b}{b : T_2}{a : T_1 \oplus T_2}}
         {}
   \\[1mm]
   \infer{\proctype{p_1; p_2}{\Gamma_1, \Gamma_2}{\Delta_1, \Delta_2}}
         {\proctype{p1}{\Gamma_1}{\Xi, \Delta_1}
           \;\;\;\;
           \proctype{p2}{\Gamma_2, \Xi}{\Delta_2}
         }
  \end{gather*}
\end{minipage}}
  \caption{Type system for procedure language}
\end{figure*}

\begin{figure*}
  \fbox{\begin{minipage}{\columnwidth}
      {``$y = 5 + 6$''} \\
\[
 \begin{array}{rcl}
   x_1 & = & Const \; 5 \\
   x_2 & = & Const \; 6 \\
   t  & = & (x_1, x_2) \\
   y  & = & Call \; add \; t \\
   \vdash & \cdot & \dashv y : \real \\
 \end{array}
\]
  \end{minipage}}
  \fbox{\begin{minipage}{\columnwidth}
      {``$(x, y) = (r \cos \theta, r \sin \theta)$''} \\
\[
 \begin{array}{rcl}
   (r_1, r_2) & = & Dup \; r \\
   (\theta_1, \theta_2) & = & Dup \; \theta \\
   ct  & = & Call \; cos \; \theta_1 \\
   st  & = & Call \; sin \; \theta_2 \\
   rct & = & (r_1, ct) \\
   rst & = & (r_2, st) \\
   x & = & Call \; mul \; rct \\
   y & = & Call \; mul \; rst \\
   r : \real, \; \theta : \real \vdash & \cdot & \dashv x : \real, \; y : \real \\
 \end{array}
\]
  \end{minipage}}
  \fbox{\begin{minipage}{\columnwidth}
      {``$r = x^2 + \sin{xy} + 1$''} \\
\[
 \begin{array}{rcl}
   (x_1, x_2) & = & Dup \; x \\
   (x_3, x_4)  & = & Dup \; x_1 \\
   xx  & = & (x_2, x_3) \\
   xsq  & = & Call \; mul \; xx \\
   xy   & = & (x_4, y) \\
   xmuly & = & Call \; mul \; xy \\
   sinxy & = & Call \; sin \; xmuly \\
   o & = & Const \; 1 \\
   sinxy1 & = & Call \; add \; sinxy \; o \\
   r & = & Call \; add \; xsq \; sinxy1 \\
   x : \real, \; y : \real \vdash & \cdot & \dashv r : \real \\
 \end{array}
\]
  \end{minipage}}
  \caption{Example procedures}
\end{figure*}

% Tangent types. Upsilon looks a bit like a T.
\newcommand{\TT}{\Upsilon}
\newcommand{\emptycontext}{\emptyset}
\newcommand{\fwdsplit}[1]{F \lb #1 \rb}
\newcommand{\revsplit}[1]{R \lb #1 \rb}

\begin{figure*}
  \fbox{\begin{minipage}{\textwidth}
$$
      \begin{array}{c|rcl|c|rcl|c|rcl}
%        \multicolumn{12}{c}{\mbox{\bf Reverse mode}} \\

        \Gamma &
        \multicolumn{3}{c|}{\proctype{p}{\Gamma}{\Delta}} &
        \Delta &
        \multicolumn{3}{c|}{\proctype{\fwdsplit{p}}{\Gamma}{\Delta, \TT_p}} &
        \TT_p &
        \multicolumn{3}{c}{\proctype{\revsplit{p}}{\bar{\Delta}, \TT_p}{\bar{\Gamma}}} \\

        \hline

        a &
        b & = & Call \; f \; a &
        b &
        bt & = & Call \; rf\$f \; a &
        t_{b;a} &
        bt' & = & (\bar{b}, t_{b;a}) \\

        &
        & & &
        &
        (b, t_{b;a}) & = & bt &
        &
        \bar{a} & = & Call \; rr\$f \; bt' \\

        \hline

        s &
        t & = & Dup \; s &
        t &
        t & = & Dup \; s &
        \emptycontext &
        \bar{s} & = & Call \; add \; \bar{t} \\

        \hline

        \emptycontext &
        a & = & Const \; k &
        a &
        a & = & Const \; k &
        \emptycontext &
        & & Elim \; \bar{a} \\

        \hline

        a &
        & & Elim \; a &
        &
        & & Elim \; a &
        \emptycontext &
        \bar{a} & = & Const \; 0 \\

        \hline

        a_1, a_2 &
        b & = & (a_1, a_2) &
        b &
        b & = & (a_1, a_2) &
        \emptycontext &
        (\bar{a}_1, \bar{a}_2) & = & \bar{b} \\

        \hline

        b &
        (a_1, a_2) & = & b &
        a_1, a_2 &
        (a_1, a_2) & = & b &
        \emptycontext &
        \bar{b} & = & (\bar{a}_1, \bar{a}_2) \\

        \hline

        b &
        a & = & Inl \; b &
        a &
        a & = & Inl \; b &
        \emptycontext &
        \multicolumn{3}{c}{Case \; \bar{a}} \\

        & & & & & & & & &

        \multicolumn{3}{c}{(Inl \; \bar{b})} \\

        & & & & & & & & &

        \multicolumn{3}{c}{(Inr \; \bar{z} \; (Elim \; \bar{z}; \bar{b} = Const \; 0))} \\

        \hline

        b &
        a & = & Inr \; b &
        a &
        a & = & Inr \; b &
        \emptycontext &
        \multicolumn{3}{c}{Case \; \bar{a}} \\

        & & & & & & & & &

        \multicolumn{3}{c}{(Inl \; \bar{z} \; (Elim \; \bar{z}; \bar{b} = Const \; 0))} \\

        & & & & & & & & &

        \multicolumn{3}{c}{(Inr \; \bar{b})} \\

        \hline

        %% \Gamma_1 &
        %% \multicolumn{3}{c|}{p1} &
        %% \Delta_1, \Xi &
        %% \multicolumn{3}{c|}{rf\$p1} &
        %% \TT_{p1} &
        %% \multicolumn{3}{c}{rr\$p1} \\

        %% \hline

        %% \Gamma_2, \Xi &
        %% \multicolumn{3}{c|}{p2} &
        %% \Delta_2 &
        %% \multicolumn{3}{c|}{rf\$p2} &
        %% \TT_{p2} &
        %% \multicolumn{3}{c}{rr\$p2} \\

        %% \hline

        \Gamma_1, \Gamma_2 &
        \multicolumn{3}{c|}{p1;p2} &
        \Delta_1, \Delta_2 &
        \multicolumn{3}{c|}{\fwdsplit{p1};\fwdsplit{p2}} &
        \TT_{p1}, \TT_{p2} &
        \multicolumn{3}{c}{\revsplit{p2};\revsplit{p1}}
      \end{array}
$$
      \begin{minipage}{\textwidth}
        $t_{b;a}$ is a fresh variable name, $\bar{\cdot}$ bars all the
        variables in collection of variables it applies to
      \end{minipage}
\end{minipage}}
\caption{Procedure language reverse mode translation rules}
\end{figure*}


\section{BOG-style AD for ksc}

\newcommand{\fbog}[1]{#1_{\mathit{fbog}}}
\newcommand{\rbog}[1]{#1_{\mathit{rbog}}}
\newcommand{\gradfbog}[1]{\nabla_{\! f}\lb #1 \rb}  % grad[#1]
\newcommand{\gradrbog}[3]{\nabla_{\! r}\lb #1 \rb\;#2\;#3}  % grad[#1]
\newcommand{\bog}[1]{{\mathcal B}[e]}
\newcommand{\append}{+\!\!+}
\newcommand{\freevars}[1]{\mathit{fvs}(#1)}

\newenvironment{codearray}[1]
               {\renewcommand{\arraystretch}{1.0} \begin{array}[t]{@{}#1}}
               {\end{array}}



\begin{figure*}
  \def\mcol#1{\multicolumn{3}{l}{#1}}
  \def\mkhead#1{\multicolumn{3}{l}{\begin{minipage}[t]{0.45\columnwidth}#1\end{minipage}}}
  \def\eq<#1===#2>{$#1$   &   $=$   &    $#2$}

  \fbox{\begin{minipage}{\textwidth}
      This figure assumes the single-use dialect of \ksc{},
      in which every binder is used at most once.
$$
\begin{codearray}{ll}
  \mbox{\bf Original function}   & f : S \to T \\
  & f(s) = e \\[1mm]
\end{codearray} 
$$

\begin{tabular}[t]{rclrcl}
\mkhead{
\text{\bf Forward BOG}
\begin{align*}
 &\fbog{f} : S \to (T,\bog{e})\\
 &\fbog{f}(s) = \gradfbog{e} 
\end{align*}
If $e : T$ then $\gradfbog{e} : (T,\bog{e})$
} &
\mkhead{
\text{\bf Reverse BOG}
\begin{align*}
 &\rbog{f} : (\tangent{T},\bog{e}) \to \tangent{S} \\
 &\rbog{f}(\gradf{t},b) = \mbox{\lstinline|let $\gradrbog{e}{\gradf{t}}{b}\;$ in $\;\gradf{s}$|}
\end{align*}
If $\Gamma \vdash e : T$ and $\gradf{t}{:}\tangent{T}$ and $b{:}\bog{e}$,
      then $\gradrbog{e}{\gradf{t}}{b}$ is a set of bindings that,
       for every free variable $x{:}X$ of $e$, binds $\gradf{x}{:}\tangent{X}$
}
\\
% Constant, k
\eq<   \gradfbog{k}  === (k,())>
&
\eq<   \gradrbog{k}{\gradf{t}}{b}   === \{ \} >
\\
% Variable use, x
\eq<   \gradfbog{x}   ===  (x,()) >
&
\eq<   \gradrbog{x}{\gradf{t}}{b}    ===   \{ \gradf{x} = \gradf{t} \}   >
\\
% f(e)	
\eq<   \gradfbog{f(e)} === \begin{codearray}{l}
					          \mbox{\lstinline|let $\;(a,b_e) = \gradfbog{e}\;$ in|} \\
					          \mbox{\lstinline|let $\;(r,b_f) = \fbog{f}(a)\;$ in|} \\
					          (r, (b_1,b_2))
					          \end{codearray} 
>
&
\eq<   \gradrbog{f(e)}{\gradf{t}}{b} ===  \begin{codearray}{lll}
                                        \{ & (b_e,b_f) = b \\
                                        ;  & \gradf{a} = \rbog{f}(\gradf{t},b_f) & \} \\
                                        \multicolumn{3}{l}{\append \; \gradrbog{e}{\gradf{a}}{b_e}}
                                        \end{codearray}
>\\
% (e_1,e_2)
\eq<       \gradfbog{(e_1,e_2)}  ===  \begin{codearray}{l}
                                    \mbox{\lstinline|let $\;(a_1,b_1) = \gradfbog{e_1}\;$ in|} \\
                                    \mbox{\lstinline|let $\;(a_2,b_2) = \gradfbog{e_2}\;$ in|} \\
                                    ((a_1,a_2), (b_1,b_2))
                                    \end{codearray} >
&
\eq<       \gradrbog{(e_1,e_2)}{\gradf{t}}{b} ===
                    \begin{codearray}{lll}
                    \{ & (\gradf{t_1},\gradf{t_2}) = \gradf{t} \\
                    ;  & (b_1,b_2) = b & \} \\
                    \multicolumn{3}{l}{\append \; \gradrbog{e_1}{\gradf{t_1}}{b_1}} \\
                    \multicolumn{3}{l}{\append \; \gradrbog{e_2}{\gradf{t_2}}{b_2}}
                    \end{codearray} 
>\\
% let x = e1 in e2
\eq<       \gradfbog{\mbox{\lstinline|let $\;x$ = $e_1\;$ in $\;e_2$|}} ===
							           \begin{codearray}{l}
							           \mbox{\lstinline|let $\;(x,b_1) = \gradfbog{e_1}\;$ in|} \\
							           \mbox{\lstinline|let $\;(r,b_2) = \gradfbog{e_2}\;$ in|} \\
							           (r, (b_1,b_2))
							           \end{codearray}
>
&
\eq<       \gradrbog{\mbox{\lstinline|let $\;x$ = $e_1\;$ in $\;e_2$|}}{\gradf{t}}{b} ===
           \begin{codearray}{lll}
           \{ & (b_1,b_2) = b & \} \\
           \multicolumn{3}{l}{\append \; \gradrbog{e_2}{\gradf{t}}{b_2}} \\
           \multicolumn{3}{l}{\append \; \gradrbog{e_1}{\gradf{x}}{b_1}}
           \end{codearray}
> \\ 

% let (x, y) = e1 in e2
\eq<    \gradfbog{\mbox{\lstinline|let $\;(x,y)$ = $e_1\;$ in $\;e_2$|}} === 
		\begin{codearray}{l}
            \mbox{\lstinline|let $\;(xy,b_1) = \gradfbog{e_1}\;$ in|} \\
            \mbox{\lstinline|let $\;(x,y) = xy\;$ in|} \\
            \mbox{\lstinline|let $\;(r,b_2) = \gradfbog{e_2}\;$ in|} \\
            (r, (b_1,b_2))
            \end{codearray}
> &
\eq<
         \gradrbog{\mbox{\lstinline|let $\;(x,y)$ = $e_1\;$ in $\;e_2$|}}{\gradf{t}}{b}
          ===  \begin{codearray}{lll}
            \{ & (b_1,b_2) = b & \} \\
            \multicolumn{3}{l}{\append \; \gradrbog{e_2}{\gradf{t}}{b_2}} \\
            \multicolumn{3}{l}{\append \; \gradrbog{e_1}{(\gradf{x},\gradf{y})}{b_1}}
            \end{codearray}
> \\
% build e1 (\lambda i . e2) 
\mkhead{
\begin{multline*}
        \gradfbog{\mbox{\lstinline|build $\;e_1\;(\lambda i.\, e_2)$ |}} = \\
        \begin{codearray}{lll}
          \mbox{\lstinline|let $\;n = e_1\;$ in|} \\
          \mbox{\lstinline|let $\;(r,ba)\;$ = unzip (build $\;n\;(\lambda i.\,\gradfbog{e_2})$) in|} \\
            (r, (n,ba))
            \end{codearray}
\end{multline*}
} &
\mkhead{{}
\begin{multline*}
        \gradrbog{\mbox{\lstinline|build $\;e_1\;(\lambda i.\, e_2)$ |}}{\gradf{t}}{b} = \\
          \begin{codearray}{l@{\hspace{1mm}}l@{\hspace{1mm}}l}
            \bigl\{ & (n,ba) = b \\
            ; & \overline{v}  =
                \begin{codearray}{l}
                   \mbox{\lstinline{sumbuild n}} \\
                      \quad \mbox{\lstinline|($\lambda i.$
                      let $\;\gradrbog{e_2}{(\gradf{t}[i])}{(ba[i])}\;$ in $\;\overline{v})$|}
                \end{codearray} & \biggr\} \\
            \end{codearray}
\end{multline*}
\hspace{3em} where $\overline{v} = \freevars{e_2}$
} \\
% case e1 of { Inl x el; Inr y er }
\mkhead{
\begin{multline*}
\gradfbog{\mbox{
              \lstinline|case $\;e_1\;$ of \{ Inl x $\rightarrow e_l$; Inr y $\rightarrow e_r$ \}|}} = \\
        \begin{codearray}{l}
        \qquad   \mbox{\lstinline|case $\;e_1\;$ of|} \\
        \qquad   \quad \mbox{\lstinline|Inl $\;x \rightarrow\;$ let $\;(r_r,b_l) = \gradfbog{e_l}\;$
                                                            in $\;(r_r,\;$Inl$\;b_l)$|} \\
        \qquad  \quad \mbox{\lstinline|Inr $\;y \rightarrow\;$ let $\;(r_l,b_r) = \gradfbog{e_r}\;$
                                                            in $\;(r_l,\;$Inr$\;b_r)$|} \\
            \end{codearray}
\end{multline*}
}
& 
\mkhead{
\begin{multline*}
          \gradrbog{\mbox{\lstinline|case $\;e_1\;$ of
              \{ Inl x $\rightarrow e_2$; Inr y $\rightarrow e_3$ \}|}}{\gradf{t}}{b} =\\
          \begin{codearray}{l}
           \{\, (b_1,b_2) = b \\
            ;\, (\gradf{xy}, \overline{v}) = \begin{codearray}{l}
         \mbox{\lstinline|case $\;b_2\;$ of|} \\
         \quad \mbox{\lstinline|Inl $\;b_l\rightarrow \;$ let $\;\gradrbog{e_l}{\gradf{t}}{b_l}\;$ in $\;($Inl$\;\gradf{x},\overline{v})$|} \\
         \quad \mbox{\lstinline|Inr $\;b_r\rightarrow \;$ let $\;\gradrbog{e_r}{\gradf{t}}{b_r}\;$ in $\;($Inr$\;\gradf{y},\overline{v})$|}\, \} \\
           \end{codearray} \\
         \append \; \gradrbog{e_1}{\gradf{xy}}{b_1}
         \end{codearray}
    \end{multline*}
}\\
\end{tabular}
\end{minipage}}
\caption{BOG-style AD for ksc} \label{fig:ksc-bog-ad-fwd}
\end{figure*}


See Figures~\ref{fig:ksc-bog-ad-fwd} and \ref{fig:ksc-bog-ad-rev} for BOG-style AD for \ksc.  This Figure
assumes that the input language is a \emph{single-use dialect} of \ksc,
in which every binder is used exactly once.  (Or at most once, I'm not quite sure yet.)

Where is this assumption used?   The reverse-pass transformation $\gradrbog{e}$ creates
a let-binding for every occurrence of a variable in $e$.  If any variable occurs more than
once, these bindings will conflict, or (equally bad) one will shadow the other so
that the earlier one is ignored.  Instead we rely on uses of \lstinline|dup| to
duplicate binders; in the reverse pass $\rbog{\lstinline|dup|}$ adds up the
contribution of the two occurrences.

Note: In place of $\bog{e}$ (a data structure) we could use a partial application of $e_{rbog}(\bog{e}) : \tangent{T} \rightarrow \tangent{S}$.


\end{document}

\section{Old stuff about Build and lambda} \label{sec:build-lam-fail}

I'm not happy with the story for build and lambda. It started well:
\begin{itemize}
\item For a start, the treatment of lambda is very special: it works only
  for $(\nat \to t)$, which seems oddly assymetrical.

\item The AD for build (Fig~\ref{fig:ad}) looks sensible.
  But not great:  rather than use the generic case for a call $f(e)$,
  I used a special case for $\grad{\buildfun(e_n,e)}$; and I did
  an ad-hoc thing of not AD'ing the first argument to build, which
  seems very arbitrary.  The AD's version of build uses $\lmbuild$, whose
  signature is in Figure~\ref{fig:linear-maps}; and whose semantics is
  defined by how it behaves when applied (Figure~\ref{fig:lm-laws}).

\item The AD for lambda (Figure~\ref{fig:ad}) looks sensible.  It generates
  a new linear map $\lmlam$, whose
  signature is in Figure~\ref{fig:linear-maps}; and whose semantics is
  defined by how it behaves when applied (Figure~\ref{fig:lm-laws}).

\item But then things get trickier.  What is the transose of $\lmlam$?
  I invented $\lmlamt$ (Figure~\ref{fig:linear-maps}) as its transpose.
  There is a nice pattern here: $\lmvcat$ and $\lmhcat$ are related
  in just the same way as $\lmlam$ and $\lmlamt$.

\item But I got stuck: what is the semantics of $\lmlamt$?
  There is a stab in Figure~\ref{fig:lm-laws}, but the $\Sigma_i$ is deeply
  suspicious, because it doesn't give the range of $i$.  That range comes
  from the enclosing build.
\end{itemize}

An alternative is to replace lambda with
$$
e ::= \ldots | \buildfun(n) i e
$$
where $i$ is a variable (of type $\nat$) that scopes over $e$.
So $\buildfun(n)~ i~ e$ is what we have been writing $\buildfun(n, \lambda i.e)$.
This looks simpler and more direct to me.
\tom{Is there something to stop me writing $let f = \lambda i.e~ in~
  \buildfun(n)~ i~ f(i)$ in an attempt to resurrect original behaviour?}
\simon{Yes: $f(i)$ requires $f$ to be top level -- see Section 2.  But
  the solution I adopted was not to have a new language construct, but
  rather to differentiate only $\buildfun(n, \lambda i.e)$.  Any other
  use of $\buildfun$ will fail.}



\end{document}

do  C( s -o t) -> s -o C(t)
un             -> C(s) -o t

Given    m :: s -o (N -> t)
Wanted   mt :: (N -> t) -o s

L  :: (N -> (s -o t)) -> s -o (N -> t)
Lt ::                 -> (N -> s) -o t
