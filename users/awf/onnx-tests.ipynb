{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super Resolution model definition in PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv4.weight)\n",
    "\n",
    "# Create the super-resolution model by using the above model definition.\n",
    "torch_model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth\" to /home/awf/.cache/torch/checkpoints/superres_epoch100-44c6958e.pth\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=239691.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e172ea264bd6429fa6f071645c4e1e3b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SuperResolutionNet(\n",
       "  (relu): ReLU()\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pixel_shuffle): PixelShuffle(upscale_factor=3)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Load pretrained model weights\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "batch_size = 1    # just a random number\n",
    "\n",
    "# Initialize model with the pretrained weights\n",
    "map_location = lambda storage, loc: storage\n",
    "if torch.cuda.is_available():\n",
    "    map_location = None\n",
    "torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n",
    "\n",
    "# set the model to inference mode\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.18146938375342359"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Input to the model\n",
    "x = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n",
    "torch_out = torch_model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(torch_model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"super_resolution_noparams.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=False,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                                'output' : {0 : 'batch_size'}})\n",
    "import random\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "graph torch-jit-export (\n  %input[FLOAT, batch_sizex1x224x224]\n) initializers (\n  %conv1.bias[FLOAT, 64]\n  %conv1.weight[FLOAT, 64x1x5x5]\n  %conv2.bias[FLOAT, 64]\n  %conv2.weight[FLOAT, 64x64x3x3]\n  %conv3.bias[FLOAT, 32]\n  %conv3.weight[FLOAT, 32x64x3x3]\n  %conv4.bias[FLOAT, 9]\n  %conv4.weight[FLOAT, 9x32x3x3]\n) {\n  %9 = Conv[dilations = [1, 1], group = 1, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%input, %conv1.weight, %conv1.bias)\n  %10 = Relu(%9)\n  %11 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%10, %conv2.weight, %conv2.bias)\n  %12 = Relu(%11)\n  %13 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%12, %conv3.weight, %conv3.bias)\n  %14 = Relu(%13)\n  %15 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%14, %conv4.weight, %conv4.bias)\n  %16 = Constant[value = <Tensor>]()\n  %17 = Reshape(%15, %16)\n  %18 = Transpose[perm = [0, 1, 4, 2, 5, 3]](%17)\n  %19 = Constant[value = <Tensor>]()\n  %output = Reshape(%18, %19)\n  return %output\n}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "model = onnx.load(\"../../test/onnx2k/super_resolution.onnx\")\n",
    "onnx.checker.check_model(model)\n",
    "print(onnx.helper.printable_graph(model.graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-m0skcp_r\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-m0skcp_r\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: sentencepiece==0.1.91 in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (0.1.91)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (4.47.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers==0.9.2 in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (0.9.2)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: protobuf in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (3.13.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (2020.6.8)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from transformers==3.4.0) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from packaging->transformers==3.4.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from protobuf->transformers==3.4.0) (47.3.1.post20200622)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from requests->transformers==3.4.0) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from requests->transformers==3.4.0) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from requests->transformers==3.4.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from requests->transformers==3.4.0) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: click in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /anaconda/envs/py37_pytorch/lib/python3.7/site-packages (from sacremoses->transformers==3.4.0) (0.15.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-3.4.0-py3-none-any.whl size=1296698 sha256=1f39329d66047b20ed5bf5eba6316b17e046b64fe985ec07e2a15f4e1c23bbc8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-psqkrxyg/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 3.4.0\n",
      "    Uninstalling transformers-3.4.0:\n",
      "      Successfully uninstalled transformers-3.4.0\n",
      "Successfully installed transformers-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading pipeline (model: bert-base-cased, tokenizer: bert-base-cased)\n",
      "Using framework PyTorch: 1.5.1\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n",
      "total 423160\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1661\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1662\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1663\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1671\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1673\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1674\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1676\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1677\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1678\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1686\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1688\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1689\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1691\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1692\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1693\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1701\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1703\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1704\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1706\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1707\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1708\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1716\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1718\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1719\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1721\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1722\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1723\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1731\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1733\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1734\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1736\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1737\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1738\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1746\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1748\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1749\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1751\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1752\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1753\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1761\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1763\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1764\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1766\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1767\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1768\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1776\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1778\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1779\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1781\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1782\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1783\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1791\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1793\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1794\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1796\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1797\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1798\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1806\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1808\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1809\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1811\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1812\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1813\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1821\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1823\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1824\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1826\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1827\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1828\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 1836\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1838\n",
      "-rw-rw-r-- 1 awf awf  9437184 Nov  6 17:40 1839\n",
      "-rw-rw-r-- 1 awf awf  1572864 Nov  6 17:40 embeddings.position_embeddings.weight\n",
      "-rw-rw-r-- 1 awf awf     6144 Nov  6 17:40 embeddings.token_type_embeddings.weight\n",
      "-rw-rw-r-- 1 awf awf 89075712 Nov  6 17:40 embeddings.word_embeddings.weight\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.0.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.1.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.10.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.11.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.2.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.3.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.4.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.5.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.6.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.7.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.8.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf    12288 Nov  6 17:40 encoder.layer.9.intermediate.dense.bias\n",
      "-rw-rw-r-- 1 awf awf   413422 Nov  6 17:40 file.onnx\n",
      "-rw-rw-r-- 1 awf awf  2359296 Nov  6 17:40 pooler.dense.weight\n"
     ]
    }
   ],
   "source": [
    "from transformers.convert_graph_to_onnx import load_graph_from_args,convert_pytorch\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = load_graph_from_args(\"feature-extraction\", \"pt\", \"bert-base-cased\")\n",
    "!mkdir /tmp/bert-base-cased\n",
    "convert_pytorch(nlp, opset=11, output=Path(\"/tmp/bert-base-cased/file.onnx\"), use_external_format=True)\n",
    "!ls -l /tmp/bert-base-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1030 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1031 = Add(%1029, %1030)\n",
      "  %1032 = Sqrt(%1031)\n",
      "  %1033 = Div(%1025, %1032)\n",
      "  %1034 = Mul(%1033, %encoder.layer.6.attention.output.LayerNorm.weight)\n",
      "  %1035 = Add(%1034, %encoder.layer.6.attention.output.LayerNorm.bias)\n",
      "  %1037 = MatMul(%1035, %1763)\n",
      "  %1038 = Add(%1037, %encoder.layer.6.intermediate.dense.bias)\n",
      "  %1039 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1040 = Div(%1038, %1039)\n",
      "  %1041 = Erf(%1040)\n",
      "  %1042 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1043 = Add(%1041, %1042)\n",
      "  %1044 = Mul(%1038, %1043)\n",
      "  %1045 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1046 = Mul(%1044, %1045)\n",
      "  %1048 = MatMul(%1046, %1764)\n",
      "  %1049 = Add(%1048, %encoder.layer.6.output.dense.bias)\n",
      "  %1050 = Add(%1049, %1035)\n",
      "  %1052 = ReduceMean[axes = [-1]](%1050)\n",
      "  %1053 = Sub(%1050, %1052)\n",
      "  %1054 = Cast[to = 1](%1053)\n",
      "  %1056 = Pow(%1054, %1765)\n",
      "  %1057 = ReduceMean[axes = [-1]](%1056)\n",
      "  %1058 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1059 = Add(%1057, %1058)\n",
      "  %1060 = Sqrt(%1059)\n",
      "  %1061 = Div(%1053, %1060)\n",
      "  %1062 = Mul(%1061, %encoder.layer.6.output.LayerNorm.weight)\n",
      "  %1063 = Add(%1062, %encoder.layer.6.output.LayerNorm.bias)\n",
      "  %1065 = MatMul(%1063, %1766)\n",
      "  %1066 = Add(%1065, %encoder.layer.7.attention.self.query.bias)\n",
      "  %1068 = MatMul(%1063, %1767)\n",
      "  %1069 = Add(%1068, %encoder.layer.7.attention.self.key.bias)\n",
      "  %1071 = MatMul(%1063, %1768)\n",
      "  %1072 = Add(%1071, %encoder.layer.7.attention.self.value.bias)\n",
      "  %1073 = Shape(%1066)\n",
      "  %1074 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1075 = Gather[axis = 0](%1073, %1074)\n",
      "  %1076 = Shape(%1066)\n",
      "  %1077 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1078 = Gather[axis = 0](%1076, %1077)\n",
      "  %1081 = Unsqueeze[axes = [0]](%1075)\n",
      "  %1082 = Unsqueeze[axes = [0]](%1078)\n",
      "  %1085 = Concat[axis = 0](%1081, %1082, %1769, %1770)\n",
      "  %1086 = Reshape(%1066, %1085)\n",
      "  %1087 = Transpose[perm = [0, 2, 1, 3]](%1086)\n",
      "  %1088 = Shape(%1069)\n",
      "  %1089 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1090 = Gather[axis = 0](%1088, %1089)\n",
      "  %1091 = Shape(%1069)\n",
      "  %1092 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1093 = Gather[axis = 0](%1091, %1092)\n",
      "  %1096 = Unsqueeze[axes = [0]](%1090)\n",
      "  %1097 = Unsqueeze[axes = [0]](%1093)\n",
      "  %1100 = Concat[axis = 0](%1096, %1097, %1771, %1772)\n",
      "  %1101 = Reshape(%1069, %1100)\n",
      "  %1102 = Shape(%1072)\n",
      "  %1103 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1104 = Gather[axis = 0](%1102, %1103)\n",
      "  %1105 = Shape(%1072)\n",
      "  %1106 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1107 = Gather[axis = 0](%1105, %1106)\n",
      "  %1110 = Unsqueeze[axes = [0]](%1104)\n",
      "  %1111 = Unsqueeze[axes = [0]](%1107)\n",
      "  %1114 = Concat[axis = 0](%1110, %1111, %1773, %1774)\n",
      "  %1115 = Reshape(%1072, %1114)\n",
      "  %1116 = Transpose[perm = [0, 2, 1, 3]](%1115)\n",
      "  %1117 = Transpose[perm = [0, 2, 3, 1]](%1101)\n",
      "  %1118 = MatMul(%1087, %1117)\n",
      "  %1119 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1120 = Div(%1118, %1119)\n",
      "  %1121 = Add(%1120, %209)\n",
      "  %1122 = Softmax[axis = 3](%1121)\n",
      "  %1123 = MatMul(%1122, %1116)\n",
      "  %1124 = Transpose[perm = [0, 2, 1, 3]](%1123)\n",
      "  %1125 = Shape(%1124)\n",
      "  %1126 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1127 = Gather[axis = 0](%1125, %1126)\n",
      "  %1128 = Shape(%1124)\n",
      "  %1129 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1130 = Gather[axis = 0](%1128, %1129)\n",
      "  %1132 = Unsqueeze[axes = [0]](%1127)\n",
      "  %1133 = Unsqueeze[axes = [0]](%1130)\n",
      "  %1135 = Concat[axis = 0](%1132, %1133, %1775)\n",
      "  %1136 = Reshape(%1124, %1135)\n",
      "  %1138 = MatMul(%1136, %1776)\n",
      "  %1139 = Add(%1138, %encoder.layer.7.attention.output.dense.bias)\n",
      "  %1140 = Add(%1139, %1063)\n",
      "  %1142 = ReduceMean[axes = [-1]](%1140)\n",
      "  %1143 = Sub(%1140, %1142)\n",
      "  %1144 = Cast[to = 1](%1143)\n",
      "  %1146 = Pow(%1144, %1777)\n",
      "  %1147 = ReduceMean[axes = [-1]](%1146)\n",
      "  %1148 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1149 = Add(%1147, %1148)\n",
      "  %1150 = Sqrt(%1149)\n",
      "  %1151 = Div(%1143, %1150)\n",
      "  %1152 = Mul(%1151, %encoder.layer.7.attention.output.LayerNorm.weight)\n",
      "  %1153 = Add(%1152, %encoder.layer.7.attention.output.LayerNorm.bias)\n",
      "  %1155 = MatMul(%1153, %1778)\n",
      "  %1156 = Add(%1155, %encoder.layer.7.intermediate.dense.bias)\n",
      "  %1157 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1158 = Div(%1156, %1157)\n",
      "  %1159 = Erf(%1158)\n",
      "  %1160 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1161 = Add(%1159, %1160)\n",
      "  %1162 = Mul(%1156, %1161)\n",
      "  %1163 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1164 = Mul(%1162, %1163)\n",
      "  %1166 = MatMul(%1164, %1779)\n",
      "  %1167 = Add(%1166, %encoder.layer.7.output.dense.bias)\n",
      "  %1168 = Add(%1167, %1153)\n",
      "  %1170 = ReduceMean[axes = [-1]](%1168)\n",
      "  %1171 = Sub(%1168, %1170)\n",
      "  %1172 = Cast[to = 1](%1171)\n",
      "  %1174 = Pow(%1172, %1780)\n",
      "  %1175 = ReduceMean[axes = [-1]](%1174)\n",
      "  %1176 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1177 = Add(%1175, %1176)\n",
      "  %1178 = Sqrt(%1177)\n",
      "  %1179 = Div(%1171, %1178)\n",
      "  %1180 = Mul(%1179, %encoder.layer.7.output.LayerNorm.weight)\n",
      "  %1181 = Add(%1180, %encoder.layer.7.output.LayerNorm.bias)\n",
      "  %1183 = MatMul(%1181, %1781)\n",
      "  %1184 = Add(%1183, %encoder.layer.8.attention.self.query.bias)\n",
      "  %1186 = MatMul(%1181, %1782)\n",
      "  %1187 = Add(%1186, %encoder.layer.8.attention.self.key.bias)\n",
      "  %1189 = MatMul(%1181, %1783)\n",
      "  %1190 = Add(%1189, %encoder.layer.8.attention.self.value.bias)\n",
      "  %1191 = Shape(%1184)\n",
      "  %1192 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1193 = Gather[axis = 0](%1191, %1192)\n",
      "  %1194 = Shape(%1184)\n",
      "  %1195 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1196 = Gather[axis = 0](%1194, %1195)\n",
      "  %1199 = Unsqueeze[axes = [0]](%1193)\n",
      "  %1200 = Unsqueeze[axes = [0]](%1196)\n",
      "  %1203 = Concat[axis = 0](%1199, %1200, %1784, %1785)\n",
      "  %1204 = Reshape(%1184, %1203)\n",
      "  %1205 = Transpose[perm = [0, 2, 1, 3]](%1204)\n",
      "  %1206 = Shape(%1187)\n",
      "  %1207 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1208 = Gather[axis = 0](%1206, %1207)\n",
      "  %1209 = Shape(%1187)\n",
      "  %1210 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1211 = Gather[axis = 0](%1209, %1210)\n",
      "  %1214 = Unsqueeze[axes = [0]](%1208)\n",
      "  %1215 = Unsqueeze[axes = [0]](%1211)\n",
      "  %1218 = Concat[axis = 0](%1214, %1215, %1786, %1787)\n",
      "  %1219 = Reshape(%1187, %1218)\n",
      "  %1220 = Shape(%1190)\n",
      "  %1221 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1222 = Gather[axis = 0](%1220, %1221)\n",
      "  %1223 = Shape(%1190)\n",
      "  %1224 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1225 = Gather[axis = 0](%1223, %1224)\n",
      "  %1228 = Unsqueeze[axes = [0]](%1222)\n",
      "  %1229 = Unsqueeze[axes = [0]](%1225)\n",
      "  %1232 = Concat[axis = 0](%1228, %1229, %1788, %1789)\n",
      "  %1233 = Reshape(%1190, %1232)\n",
      "  %1234 = Transpose[perm = [0, 2, 1, 3]](%1233)\n",
      "  %1235 = Transpose[perm = [0, 2, 3, 1]](%1219)\n",
      "  %1236 = MatMul(%1205, %1235)\n",
      "  %1237 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1238 = Div(%1236, %1237)\n",
      "  %1239 = Add(%1238, %209)\n",
      "  %1240 = Softmax[axis = 3](%1239)\n",
      "  %1241 = MatMul(%1240, %1234)\n",
      "  %1242 = Transpose[perm = [0, 2, 1, 3]](%1241)\n",
      "  %1243 = Shape(%1242)\n",
      "  %1244 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1245 = Gather[axis = 0](%1243, %1244)\n",
      "  %1246 = Shape(%1242)\n",
      "  %1247 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1248 = Gather[axis = 0](%1246, %1247)\n",
      "  %1250 = Unsqueeze[axes = [0]](%1245)\n",
      "  %1251 = Unsqueeze[axes = [0]](%1248)\n",
      "  %1253 = Concat[axis = 0](%1250, %1251, %1790)\n",
      "  %1254 = Reshape(%1242, %1253)\n",
      "  %1256 = MatMul(%1254, %1791)\n",
      "  %1257 = Add(%1256, %encoder.layer.8.attention.output.dense.bias)\n",
      "  %1258 = Add(%1257, %1181)\n",
      "  %1260 = ReduceMean[axes = [-1]](%1258)\n",
      "  %1261 = Sub(%1258, %1260)\n",
      "  %1262 = Cast[to = 1](%1261)\n",
      "  %1264 = Pow(%1262, %1792)\n",
      "  %1265 = ReduceMean[axes = [-1]](%1264)\n",
      "  %1266 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1267 = Add(%1265, %1266)\n",
      "  %1268 = Sqrt(%1267)\n",
      "  %1269 = Div(%1261, %1268)\n",
      "  %1270 = Mul(%1269, %encoder.layer.8.attention.output.LayerNorm.weight)\n",
      "  %1271 = Add(%1270, %encoder.layer.8.attention.output.LayerNorm.bias)\n",
      "  %1273 = MatMul(%1271, %1793)\n",
      "  %1274 = Add(%1273, %encoder.layer.8.intermediate.dense.bias)\n",
      "  %1275 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1276 = Div(%1274, %1275)\n",
      "  %1277 = Erf(%1276)\n",
      "  %1278 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1279 = Add(%1277, %1278)\n",
      "  %1280 = Mul(%1274, %1279)\n",
      "  %1281 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1282 = Mul(%1280, %1281)\n",
      "  %1284 = MatMul(%1282, %1794)\n",
      "  %1285 = Add(%1284, %encoder.layer.8.output.dense.bias)\n",
      "  %1286 = Add(%1285, %1271)\n",
      "  %1288 = ReduceMean[axes = [-1]](%1286)\n",
      "  %1289 = Sub(%1286, %1288)\n",
      "  %1290 = Cast[to = 1](%1289)\n",
      "  %1292 = Pow(%1290, %1795)\n",
      "  %1293 = ReduceMean[axes = [-1]](%1292)\n",
      "  %1294 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1295 = Add(%1293, %1294)\n",
      "  %1296 = Sqrt(%1295)\n",
      "  %1297 = Div(%1289, %1296)\n",
      "  %1298 = Mul(%1297, %encoder.layer.8.output.LayerNorm.weight)\n",
      "  %1299 = Add(%1298, %encoder.layer.8.output.LayerNorm.bias)\n",
      "  %1301 = MatMul(%1299, %1796)\n",
      "  %1302 = Add(%1301, %encoder.layer.9.attention.self.query.bias)\n",
      "  %1304 = MatMul(%1299, %1797)\n",
      "  %1305 = Add(%1304, %encoder.layer.9.attention.self.key.bias)\n",
      "  %1307 = MatMul(%1299, %1798)\n",
      "  %1308 = Add(%1307, %encoder.layer.9.attention.self.value.bias)\n",
      "  %1309 = Shape(%1302)\n",
      "  %1310 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1311 = Gather[axis = 0](%1309, %1310)\n",
      "  %1312 = Shape(%1302)\n",
      "  %1313 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1314 = Gather[axis = 0](%1312, %1313)\n",
      "  %1317 = Unsqueeze[axes = [0]](%1311)\n",
      "  %1318 = Unsqueeze[axes = [0]](%1314)\n",
      "  %1321 = Concat[axis = 0](%1317, %1318, %1799, %1800)\n",
      "  %1322 = Reshape(%1302, %1321)\n",
      "  %1323 = Transpose[perm = [0, 2, 1, 3]](%1322)\n",
      "  %1324 = Shape(%1305)\n",
      "  %1325 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1326 = Gather[axis = 0](%1324, %1325)\n",
      "  %1327 = Shape(%1305)\n",
      "  %1328 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1329 = Gather[axis = 0](%1327, %1328)\n",
      "  %1332 = Unsqueeze[axes = [0]](%1326)\n",
      "  %1333 = Unsqueeze[axes = [0]](%1329)\n",
      "  %1336 = Concat[axis = 0](%1332, %1333, %1801, %1802)\n",
      "  %1337 = Reshape(%1305, %1336)\n",
      "  %1338 = Shape(%1308)\n",
      "  %1339 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1340 = Gather[axis = 0](%1338, %1339)\n",
      "  %1341 = Shape(%1308)\n",
      "  %1342 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1343 = Gather[axis = 0](%1341, %1342)\n",
      "  %1346 = Unsqueeze[axes = [0]](%1340)\n",
      "  %1347 = Unsqueeze[axes = [0]](%1343)\n",
      "  %1350 = Concat[axis = 0](%1346, %1347, %1803, %1804)\n",
      "  %1351 = Reshape(%1308, %1350)\n",
      "  %1352 = Transpose[perm = [0, 2, 1, 3]](%1351)\n",
      "  %1353 = Transpose[perm = [0, 2, 3, 1]](%1337)\n",
      "  %1354 = MatMul(%1323, %1353)\n",
      "  %1355 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1356 = Div(%1354, %1355)\n",
      "  %1357 = Add(%1356, %209)\n",
      "  %1358 = Softmax[axis = 3](%1357)\n",
      "  %1359 = MatMul(%1358, %1352)\n",
      "  %1360 = Transpose[perm = [0, 2, 1, 3]](%1359)\n",
      "  %1361 = Shape(%1360)\n",
      "  %1362 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1363 = Gather[axis = 0](%1361, %1362)\n",
      "  %1364 = Shape(%1360)\n",
      "  %1365 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1366 = Gather[axis = 0](%1364, %1365)\n",
      "  %1368 = Unsqueeze[axes = [0]](%1363)\n",
      "  %1369 = Unsqueeze[axes = [0]](%1366)\n",
      "  %1371 = Concat[axis = 0](%1368, %1369, %1805)\n",
      "  %1372 = Reshape(%1360, %1371)\n",
      "  %1374 = MatMul(%1372, %1806)\n",
      "  %1375 = Add(%1374, %encoder.layer.9.attention.output.dense.bias)\n",
      "  %1376 = Add(%1375, %1299)\n",
      "  %1378 = ReduceMean[axes = [-1]](%1376)\n",
      "  %1379 = Sub(%1376, %1378)\n",
      "  %1380 = Cast[to = 1](%1379)\n",
      "  %1382 = Pow(%1380, %1807)\n",
      "  %1383 = ReduceMean[axes = [-1]](%1382)\n",
      "  %1384 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1385 = Add(%1383, %1384)\n",
      "  %1386 = Sqrt(%1385)\n",
      "  %1387 = Div(%1379, %1386)\n",
      "  %1388 = Mul(%1387, %encoder.layer.9.attention.output.LayerNorm.weight)\n",
      "  %1389 = Add(%1388, %encoder.layer.9.attention.output.LayerNorm.bias)\n",
      "  %1391 = MatMul(%1389, %1808)\n",
      "  %1392 = Add(%1391, %encoder.layer.9.intermediate.dense.bias)\n",
      "  %1393 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1394 = Div(%1392, %1393)\n",
      "  %1395 = Erf(%1394)\n",
      "  %1396 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1397 = Add(%1395, %1396)\n",
      "  %1398 = Mul(%1392, %1397)\n",
      "  %1399 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1400 = Mul(%1398, %1399)\n",
      "  %1402 = MatMul(%1400, %1809)\n",
      "  %1403 = Add(%1402, %encoder.layer.9.output.dense.bias)\n",
      "  %1404 = Add(%1403, %1389)\n",
      "  %1406 = ReduceMean[axes = [-1]](%1404)\n",
      "  %1407 = Sub(%1404, %1406)\n",
      "  %1408 = Cast[to = 1](%1407)\n",
      "  %1410 = Pow(%1408, %1810)\n",
      "  %1411 = ReduceMean[axes = [-1]](%1410)\n",
      "  %1412 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1413 = Add(%1411, %1412)\n",
      "  %1414 = Sqrt(%1413)\n",
      "  %1415 = Div(%1407, %1414)\n",
      "  %1416 = Mul(%1415, %encoder.layer.9.output.LayerNorm.weight)\n",
      "  %1417 = Add(%1416, %encoder.layer.9.output.LayerNorm.bias)\n",
      "  %1419 = MatMul(%1417, %1811)\n",
      "  %1420 = Add(%1419, %encoder.layer.10.attention.self.query.bias)\n",
      "  %1422 = MatMul(%1417, %1812)\n",
      "  %1423 = Add(%1422, %encoder.layer.10.attention.self.key.bias)\n",
      "  %1425 = MatMul(%1417, %1813)\n",
      "  %1426 = Add(%1425, %encoder.layer.10.attention.self.value.bias)\n",
      "  %1427 = Shape(%1420)\n",
      "  %1428 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1429 = Gather[axis = 0](%1427, %1428)\n",
      "  %1430 = Shape(%1420)\n",
      "  %1431 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1432 = Gather[axis = 0](%1430, %1431)\n",
      "  %1435 = Unsqueeze[axes = [0]](%1429)\n",
      "  %1436 = Unsqueeze[axes = [0]](%1432)\n",
      "  %1439 = Concat[axis = 0](%1435, %1436, %1814, %1815)\n",
      "  %1440 = Reshape(%1420, %1439)\n",
      "  %1441 = Transpose[perm = [0, 2, 1, 3]](%1440)\n",
      "  %1442 = Shape(%1423)\n",
      "  %1443 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1444 = Gather[axis = 0](%1442, %1443)\n",
      "  %1445 = Shape(%1423)\n",
      "  %1446 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1447 = Gather[axis = 0](%1445, %1446)\n",
      "  %1450 = Unsqueeze[axes = [0]](%1444)\n",
      "  %1451 = Unsqueeze[axes = [0]](%1447)\n",
      "  %1454 = Concat[axis = 0](%1450, %1451, %1816, %1817)\n",
      "  %1455 = Reshape(%1423, %1454)\n",
      "  %1456 = Shape(%1426)\n",
      "  %1457 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1458 = Gather[axis = 0](%1456, %1457)\n",
      "  %1459 = Shape(%1426)\n",
      "  %1460 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1461 = Gather[axis = 0](%1459, %1460)\n",
      "  %1464 = Unsqueeze[axes = [0]](%1458)\n",
      "  %1465 = Unsqueeze[axes = [0]](%1461)\n",
      "  %1468 = Concat[axis = 0](%1464, %1465, %1818, %1819)\n",
      "  %1469 = Reshape(%1426, %1468)\n",
      "  %1470 = Transpose[perm = [0, 2, 1, 3]](%1469)\n",
      "  %1471 = Transpose[perm = [0, 2, 3, 1]](%1455)\n",
      "  %1472 = MatMul(%1441, %1471)\n",
      "  %1473 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1474 = Div(%1472, %1473)\n",
      "  %1475 = Add(%1474, %209)\n",
      "  %1476 = Softmax[axis = 3](%1475)\n",
      "  %1477 = MatMul(%1476, %1470)\n",
      "  %1478 = Transpose[perm = [0, 2, 1, 3]](%1477)\n",
      "  %1479 = Shape(%1478)\n",
      "  %1480 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1481 = Gather[axis = 0](%1479, %1480)\n",
      "  %1482 = Shape(%1478)\n",
      "  %1483 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1484 = Gather[axis = 0](%1482, %1483)\n",
      "  %1486 = Unsqueeze[axes = [0]](%1481)\n",
      "  %1487 = Unsqueeze[axes = [0]](%1484)\n",
      "  %1489 = Concat[axis = 0](%1486, %1487, %1820)\n",
      "  %1490 = Reshape(%1478, %1489)\n",
      "  %1492 = MatMul(%1490, %1821)\n",
      "  %1493 = Add(%1492, %encoder.layer.10.attention.output.dense.bias)\n",
      "  %1494 = Add(%1493, %1417)\n",
      "  %1496 = ReduceMean[axes = [-1]](%1494)\n",
      "  %1497 = Sub(%1494, %1496)\n",
      "  %1498 = Cast[to = 1](%1497)\n",
      "  %1500 = Pow(%1498, %1822)\n",
      "  %1501 = ReduceMean[axes = [-1]](%1500)\n",
      "  %1502 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1503 = Add(%1501, %1502)\n",
      "  %1504 = Sqrt(%1503)\n",
      "  %1505 = Div(%1497, %1504)\n",
      "  %1506 = Mul(%1505, %encoder.layer.10.attention.output.LayerNorm.weight)\n",
      "  %1507 = Add(%1506, %encoder.layer.10.attention.output.LayerNorm.bias)\n",
      "  %1509 = MatMul(%1507, %1823)\n",
      "  %1510 = Add(%1509, %encoder.layer.10.intermediate.dense.bias)\n",
      "  %1511 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1512 = Div(%1510, %1511)\n",
      "  %1513 = Erf(%1512)\n",
      "  %1514 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1515 = Add(%1513, %1514)\n",
      "  %1516 = Mul(%1510, %1515)\n",
      "  %1517 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1518 = Mul(%1516, %1517)\n",
      "  %1520 = MatMul(%1518, %1824)\n",
      "  %1521 = Add(%1520, %encoder.layer.10.output.dense.bias)\n",
      "  %1522 = Add(%1521, %1507)\n",
      "  %1524 = ReduceMean[axes = [-1]](%1522)\n",
      "  %1525 = Sub(%1522, %1524)\n",
      "  %1526 = Cast[to = 1](%1525)\n",
      "  %1528 = Pow(%1526, %1825)\n",
      "  %1529 = ReduceMean[axes = [-1]](%1528)\n",
      "  %1530 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1531 = Add(%1529, %1530)\n",
      "  %1532 = Sqrt(%1531)\n",
      "  %1533 = Div(%1525, %1532)\n",
      "  %1534 = Mul(%1533, %encoder.layer.10.output.LayerNorm.weight)\n",
      "  %1535 = Add(%1534, %encoder.layer.10.output.LayerNorm.bias)\n",
      "  %1537 = MatMul(%1535, %1826)\n",
      "  %1538 = Add(%1537, %encoder.layer.11.attention.self.query.bias)\n",
      "  %1540 = MatMul(%1535, %1827)\n",
      "  %1541 = Add(%1540, %encoder.layer.11.attention.self.key.bias)\n",
      "  %1543 = MatMul(%1535, %1828)\n",
      "  %1544 = Add(%1543, %encoder.layer.11.attention.self.value.bias)\n",
      "  %1545 = Shape(%1538)\n",
      "  %1546 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1547 = Gather[axis = 0](%1545, %1546)\n",
      "  %1548 = Shape(%1538)\n",
      "  %1549 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1550 = Gather[axis = 0](%1548, %1549)\n",
      "  %1553 = Unsqueeze[axes = [0]](%1547)\n",
      "  %1554 = Unsqueeze[axes = [0]](%1550)\n",
      "  %1557 = Concat[axis = 0](%1553, %1554, %1829, %1830)\n",
      "  %1558 = Reshape(%1538, %1557)\n",
      "  %1559 = Transpose[perm = [0, 2, 1, 3]](%1558)\n",
      "  %1560 = Shape(%1541)\n",
      "  %1561 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1562 = Gather[axis = 0](%1560, %1561)\n",
      "  %1563 = Shape(%1541)\n",
      "  %1564 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1565 = Gather[axis = 0](%1563, %1564)\n",
      "  %1568 = Unsqueeze[axes = [0]](%1562)\n",
      "  %1569 = Unsqueeze[axes = [0]](%1565)\n",
      "  %1572 = Concat[axis = 0](%1568, %1569, %1831, %1832)\n",
      "  %1573 = Reshape(%1541, %1572)\n",
      "  %1574 = Shape(%1544)\n",
      "  %1575 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1576 = Gather[axis = 0](%1574, %1575)\n",
      "  %1577 = Shape(%1544)\n",
      "  %1578 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1579 = Gather[axis = 0](%1577, %1578)\n",
      "  %1582 = Unsqueeze[axes = [0]](%1576)\n",
      "  %1583 = Unsqueeze[axes = [0]](%1579)\n",
      "  %1586 = Concat[axis = 0](%1582, %1583, %1833, %1834)\n",
      "  %1587 = Reshape(%1544, %1586)\n",
      "  %1588 = Transpose[perm = [0, 2, 1, 3]](%1587)\n",
      "  %1589 = Transpose[perm = [0, 2, 3, 1]](%1573)\n",
      "  %1590 = MatMul(%1559, %1589)\n",
      "  %1591 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1592 = Div(%1590, %1591)\n",
      "  %1593 = Add(%1592, %209)\n",
      "  %1594 = Softmax[axis = 3](%1593)\n",
      "  %1595 = MatMul(%1594, %1588)\n",
      "  %1596 = Transpose[perm = [0, 2, 1, 3]](%1595)\n",
      "  %1597 = Shape(%1596)\n",
      "  %1598 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1599 = Gather[axis = 0](%1597, %1598)\n",
      "  %1600 = Shape(%1596)\n",
      "  %1601 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1602 = Gather[axis = 0](%1600, %1601)\n",
      "  %1604 = Unsqueeze[axes = [0]](%1599)\n",
      "  %1605 = Unsqueeze[axes = [0]](%1602)\n",
      "  %1607 = Concat[axis = 0](%1604, %1605, %1835)\n",
      "  %1608 = Reshape(%1596, %1607)\n",
      "  %1610 = MatMul(%1608, %1836)\n",
      "  %1611 = Add(%1610, %encoder.layer.11.attention.output.dense.bias)\n",
      "  %1612 = Add(%1611, %1535)\n",
      "  %1614 = ReduceMean[axes = [-1]](%1612)\n",
      "  %1615 = Sub(%1612, %1614)\n",
      "  %1616 = Cast[to = 1](%1615)\n",
      "  %1618 = Pow(%1616, %1837)\n",
      "  %1619 = ReduceMean[axes = [-1]](%1618)\n",
      "  %1620 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1621 = Add(%1619, %1620)\n",
      "  %1622 = Sqrt(%1621)\n",
      "  %1623 = Div(%1615, %1622)\n",
      "  %1624 = Mul(%1623, %encoder.layer.11.attention.output.LayerNorm.weight)\n",
      "  %1625 = Add(%1624, %encoder.layer.11.attention.output.LayerNorm.bias)\n",
      "  %1627 = MatMul(%1625, %1838)\n",
      "  %1628 = Add(%1627, %encoder.layer.11.intermediate.dense.bias)\n",
      "  %1629 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1630 = Div(%1628, %1629)\n",
      "  %1631 = Erf(%1630)\n",
      "  %1632 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1633 = Add(%1631, %1632)\n",
      "  %1634 = Mul(%1628, %1633)\n",
      "  %1635 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1636 = Mul(%1634, %1635)\n",
      "  %1638 = MatMul(%1636, %1839)\n",
      "  %1639 = Add(%1638, %encoder.layer.11.output.dense.bias)\n",
      "  %1640 = Add(%1639, %1625)\n",
      "  %1642 = ReduceMean[axes = [-1]](%1640)\n",
      "  %1643 = Sub(%1640, %1642)\n",
      "  %1644 = Cast[to = 1](%1643)\n",
      "  %1646 = Pow(%1644, %1840)\n",
      "  %1647 = ReduceMean[axes = [-1]](%1646)\n",
      "  %1648 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1649 = Add(%1647, %1648)\n",
      "  %1650 = Sqrt(%1649)\n",
      "  %1651 = Div(%1643, %1650)\n",
      "  %1652 = Mul(%1651, %encoder.layer.11.output.LayerNorm.weight)\n",
      "  %output_0 = Add(%1652, %encoder.layer.11.output.LayerNorm.bias)\n",
      "  %1654 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1655 = Gather[axis = 1](%output_0, %1654)\n",
      "  %1656 = Gemm[alpha = 1, beta = 1, transB = 1](%1655, %pooler.dense.weight, %pooler.dense.bias)\n",
      "  %output_1 = Tanh(%1656)\n",
      "  return %output_0, %output_1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!echo 'FILES:' `ls ../../obj/bert-base-cased.onnx`\n",
    "model = onnx.load(\"../../obj/bert-base-cased.onnx/file.onnx\")\n",
    "#onnx.checker.check_model(model)\n",
    "s=onnx.helper.printable_graph(model.graph)\n",
    "with open('../../obj/bert-base-cased.onnx/file.txt', 'w') as f:\n",
    "    f.write(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}